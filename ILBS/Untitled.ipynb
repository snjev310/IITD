{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "listdir: embedded null character in path",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-12a5498c95ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:\\ILBS_14_8_21_Sanjeev\\ILBS_Cropped_Volumes'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\00910'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: listdir: embedded null character in path"
     ]
    }
   ],
   "source": [
    "print(os.listdir('D:\\ILBS_14_8_21_Sanjeev\\ILBS_Cropped_Volumes'+'\\00910'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.listdir(r'D:\\ILBS_14_8_21_Sanjeev\\ILBS_Cropped_Volumes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image = []\n",
    "path = 'D:\\\\ILBS_14_8_21_Sanjeev\\\\ILBS_Cropped_Volumes'\n",
    "for folder in data_folder:\n",
    "    images = []\n",
    "    IMG_HEIGHT = IMG_WIDTH =150\n",
    "    for row in os.listdir(path+'\\\\'+folder):\n",
    "        fpath = path+'\\\\'+folder+'\\\\'+row\n",
    "        #print(fpath)\n",
    "        image= cv2.imread( fpath, cv2.IMREAD_GRAYSCALE)\n",
    "        image=cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH),interpolation = cv2.INTER_AREA)\n",
    "        image=np.array(image)\n",
    "        image = image.astype('float32')\n",
    "        image /= 255\n",
    "        #image = np.expand_dims(image, axis=0)\n",
    "        images.append(image)\n",
    "    all_image.append(np.array(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = np.array(all_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 150, 150)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = merged_data[0:85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = merged_data[85::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 150, 150)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 150, 150)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = data_folder[0:85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = data_folder[85::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f = pd.read_excel('D:/ILBS_14_8_21_Sanjeev/Work_to_be_Completed_ILBS/HVPG value with CT images.xlsx',engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>HVPG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9109</td>\n",
       "      <td>2019-01-30</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19699</td>\n",
       "      <td>2019-02-13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31322</td>\n",
       "      <td>2019-02-15</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45341</td>\n",
       "      <td>2019-02-13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49565</td>\n",
       "      <td>2019-02-06</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID       Date  HVPG\n",
       "0   9109 2019-01-30    21\n",
       "1  19699 2019-02-13    12\n",
       "2  31322 2019-02-15    17\n",
       "3  45341 2019-02-13    16\n",
       "4  49565 2019-02-06    10"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "for row in df_f.values.tolist():\n",
    "    #print(row[0])\n",
    "    if row[0]=='9109':\n",
    "        row[0]='00910'\n",
    "    else:\n",
    "        row[0]=row[0]\n",
    "    #print(row[0])\n",
    "    for folder in data_folder:\n",
    "        #print((row[0]))\n",
    "        if str(row[0])==str(folder):\n",
    "            labels.append([folder,row[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.DataFrame(labels,columns=['path','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9109</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19699</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31322</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45341</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49565</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    path  label\n",
       "0   9109     21\n",
       "1  19699     12\n",
       "2  31322     17\n",
       "3  45341     16\n",
       "4  49565     10"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array(df_labels['label'][0:85])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = np.array(df_labels['label'][85::])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85,)\n",
      "(85,)\n",
      "(14,)\n",
      "(14,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 150, 150)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 150, 150)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = [np.array(x) for x in train_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = [np.array(x) for x in train_x]\n",
    "train_x_t = np.array(tr)\n",
    "te = [np.array(z) for z in test_x]\n",
    "test_x_t = np.array(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr2 = [np.array(x) for x in train_x]\n",
    "train_x_t2 = np.array(tr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = [np.array(z) for z in test_x]\n",
    "test_x_t = np.array(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_f = np.expand_dims(train_x_t,axis=4)\n",
    "#X_test_f = np.expand_dims(test_x_t, axis=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = np.array(df_train['label'])\n",
    "y_train = keras.utils.to_categorical(train_y)\n",
    "y_test = keras.utils.to_categorical(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 150, 150, 150, 1)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization, MaxPool2D\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 148, 148, 32)      43232     \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 146, 146, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 73, 73, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 341056)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               43655296  \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 28)                3612      \n",
      "=================================================================\n",
      "Total params: 43,720,636\n",
      "Trainable params: 43,720,636\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', \n",
    "                 input_shape=(150,150,150)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(28, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 76 samples, validate on 9 samples\n",
      "Epoch 1/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.0078 - accuracy: 0.5132 - val_loss: 3.0022 - val_accuracy: 0.1111\n",
      "Epoch 2/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.9451 - accuracy: 0.5395 - val_loss: 3.0294 - val_accuracy: 0.1111\n",
      "Epoch 3/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 1.9080 - accuracy: 0.5526 - val_loss: 3.0682 - val_accuracy: 0.1111\n",
      "Epoch 4/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 1.8574 - accuracy: 0.6711 - val_loss: 3.0923 - val_accuracy: 0.1111\n",
      "Epoch 5/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 1.7840 - accuracy: 0.6579 - val_loss: 3.0094 - val_accuracy: 0.1111\n",
      "Epoch 6/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 1.7457 - accuracy: 0.7237 - val_loss: 3.0360 - val_accuracy: 0.2222\n",
      "Epoch 7/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 1.6882 - accuracy: 0.7368 - val_loss: 3.0600 - val_accuracy: 0.1111\n",
      "Epoch 8/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 1.6414 - accuracy: 0.7500 - val_loss: 3.0946 - val_accuracy: 0.2222\n",
      "Epoch 9/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 1.6134 - accuracy: 0.7237 - val_loss: 3.0452 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 1.5530 - accuracy: 0.7500 - val_loss: 3.1100 - val_accuracy: 0.1111\n",
      "Epoch 11/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 1.5015 - accuracy: 0.8289 - val_loss: 3.0996 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 1.4627 - accuracy: 0.7763 - val_loss: 3.0883 - val_accuracy: 0.1111\n",
      "Epoch 13/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 1.4014 - accuracy: 0.85 - 1s 11ms/sample - loss: 1.4137 - accuracy: 0.8421 - val_loss: 3.0529 - val_accuracy: 0.1111\n",
      "Epoch 14/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 1.3722 - accuracy: 0.8421 - val_loss: 3.0344 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 1.3306 - accuracy: 0.8158 - val_loss: 3.1656 - val_accuracy: 0.1111\n",
      "Epoch 16/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 1.2936 - accuracy: 0.8553 - val_loss: 3.1955 - val_accuracy: 0.1111\n",
      "Epoch 17/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 1.2364 - accuracy: 0.8158 - val_loss: 3.1961 - val_accuracy: 0.1111\n",
      "Epoch 18/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 1.1993 - accuracy: 0.8553 - val_loss: 3.2157 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 1.1697 - accuracy: 0.8684 - val_loss: 3.1516 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 1.1233 - accuracy: 0.8421 - val_loss: 3.2231 - val_accuracy: 0.1111\n",
      "Epoch 21/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 1.0825 - accuracy: 0.8816 - val_loss: 3.1950 - val_accuracy: 0.1111\n",
      "Epoch 22/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 1.0478 - accuracy: 0.8684 - val_loss: 3.2002 - val_accuracy: 0.2222\n",
      "Epoch 23/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 1.0133 - accuracy: 0.8947 - val_loss: 3.2358 - val_accuracy: 0.1111\n",
      "Epoch 24/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.9790 - accuracy: 0.8947 - val_loss: 3.2377 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.9460 - accuracy: 0.8947 - val_loss: 3.3040 - val_accuracy: 0.2222\n",
      "Epoch 26/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.9117 - accuracy: 0.9342 - val_loss: 3.2234 - val_accuracy: 0.2222\n",
      "Epoch 27/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.8850 - accuracy: 0.9474 - val_loss: 3.2609 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.8513 - accuracy: 0.9342 - val_loss: 3.2749 - val_accuracy: 0.2222\n",
      "Epoch 29/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.8165 - accuracy: 0.9342 - val_loss: 3.2392 - val_accuracy: 0.1111\n",
      "Epoch 30/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 0.7926 - accuracy: 0.9474 - val_loss: 3.2579 - val_accuracy: 0.1111\n",
      "Epoch 31/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.7664 - accuracy: 0.9737 - val_loss: 3.2002 - val_accuracy: 0.1111\n",
      "Epoch 32/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.7369 - accuracy: 0.9474 - val_loss: 3.2104 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.7127 - accuracy: 0.9605 - val_loss: 3.2973 - val_accuracy: 0.1111\n",
      "Epoch 34/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.6882 - accuracy: 0.9737 - val_loss: 3.2596 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.6625 - accuracy: 0.9737 - val_loss: 3.2216 - val_accuracy: 0.2222\n",
      "Epoch 36/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.6368 - accuracy: 0.9868 - val_loss: 3.3250 - val_accuracy: 0.1111\n",
      "Epoch 37/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.6084 - accuracy: 0.9868 - val_loss: 3.2880 - val_accuracy: 0.1111\n",
      "Epoch 38/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.5933 - accuracy: 0.9868 - val_loss: 3.1624 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.5720 - accuracy: 0.9737 - val_loss: 3.2511 - val_accuracy: 0.1111\n",
      "Epoch 40/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.5500 - accuracy: 0.9737 - val_loss: 3.3319 - val_accuracy: 0.1111\n",
      "Epoch 41/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.5251 - accuracy: 0.9868 - val_loss: 3.3766 - val_accuracy: 0.1111\n",
      "Epoch 42/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.5070 - accuracy: 0.9868 - val_loss: 3.3526 - val_accuracy: 0.1111\n",
      "Epoch 43/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.4887 - accuracy: 0.9737 - val_loss: 3.3272 - val_accuracy: 0.1111\n",
      "Epoch 44/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.4677 - accuracy: 0.9868 - val_loss: 3.2884 - val_accuracy: 0.1111\n",
      "Epoch 45/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.4523 - accuracy: 0.9868 - val_loss: 3.3626 - val_accuracy: 0.1111\n",
      "Epoch 46/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.4397 - accuracy: 0.9868 - val_loss: 3.3805 - val_accuracy: 0.1111\n",
      "Epoch 47/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.4203 - accuracy: 0.9737 - val_loss: 3.4104 - val_accuracy: 0.1111\n",
      "Epoch 48/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.3991 - accuracy: 0.9868 - val_loss: 3.3789 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.3949 - accuracy: 0.9868 - val_loss: 3.4521 - val_accuracy: 0.1111\n",
      "Epoch 50/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.3745 - accuracy: 0.9868 - val_loss: 3.5523 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.3666 - accuracy: 0.9868 - val_loss: 3.4719 - val_accuracy: 0.1111\n",
      "Epoch 52/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.3484 - accuracy: 0.9868 - val_loss: 3.3184 - val_accuracy: 0.1111\n",
      "Epoch 53/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.3375 - accuracy: 0.9868 - val_loss: 3.4179 - val_accuracy: 0.1111\n",
      "Epoch 54/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.3208 - accuracy: 1.0000 - val_loss: 3.4940 - val_accuracy: 0.1111\n",
      "Epoch 55/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.3149 - accuracy: 0.9737 - val_loss: 3.4710 - val_accuracy: 0.1111\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.3033 - accuracy: 0.9868 - val_loss: 3.4910 - val_accuracy: 0.1111\n",
      "Epoch 57/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.2928 - accuracy: 0.9868 - val_loss: 3.5293 - val_accuracy: 0.1111\n",
      "Epoch 58/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.2794 - accuracy: 1.0000 - val_loss: 3.5483 - val_accuracy: 0.1111\n",
      "Epoch 59/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.2690 - accuracy: 0.9737 - val_loss: 3.4573 - val_accuracy: 0.1111\n",
      "Epoch 60/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.2582 - accuracy: 0.9868 - val_loss: 3.4680 - val_accuracy: 0.1111\n",
      "Epoch 61/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.2508 - accuracy: 0.9737 - val_loss: 3.4910 - val_accuracy: 0.1111\n",
      "Epoch 62/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.2401 - accuracy: 0.9737 - val_loss: 3.5480 - val_accuracy: 0.1111\n",
      "Epoch 63/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.2359 - accuracy: 0.9737 - val_loss: 3.4935 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.2222 - accuracy: 0.9868 - val_loss: 3.5745 - val_accuracy: 0.1111\n",
      "Epoch 65/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.2161 - accuracy: 0.9868 - val_loss: 3.4469 - val_accuracy: 0.2222\n",
      "Epoch 66/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.2114 - accuracy: 0.9868 - val_loss: 3.4935 - val_accuracy: 0.1111\n",
      "Epoch 67/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.1996 - accuracy: 0.9737 - val_loss: 3.6240 - val_accuracy: 0.1111\n",
      "Epoch 68/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.1899 - accuracy: 0.9868 - val_loss: 3.5648 - val_accuracy: 0.1111\n",
      "Epoch 69/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.1868 - accuracy: 0.9868 - val_loss: 3.7112 - val_accuracy: 0.1111\n",
      "Epoch 70/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.1798 - accuracy: 0.9868 - val_loss: 3.6405 - val_accuracy: 0.1111\n",
      "Epoch 71/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.1745 - accuracy: 0.9868 - val_loss: 3.6022 - val_accuracy: 0.1111\n",
      "Epoch 72/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.1697 - accuracy: 0.9868 - val_loss: 3.6649 - val_accuracy: 0.1111\n",
      "Epoch 73/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.1639 - accuracy: 0.9868 - val_loss: 3.5685 - val_accuracy: 0.1111\n",
      "Epoch 74/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.1579 - accuracy: 0.9868 - val_loss: 3.6257 - val_accuracy: 0.1111\n",
      "Epoch 75/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.1513 - accuracy: 0.9868 - val_loss: 3.5650 - val_accuracy: 0.1111\n",
      "Epoch 76/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.1454 - accuracy: 0.9868 - val_loss: 3.6606 - val_accuracy: 0.1111\n",
      "Epoch 77/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.1409 - accuracy: 0.9868 - val_loss: 3.5755 - val_accuracy: 0.2222\n",
      "Epoch 78/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.1399 - accuracy: 0.9868 - val_loss: 3.6017 - val_accuracy: 0.1111\n",
      "Epoch 79/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.1346 - accuracy: 0.9737 - val_loss: 3.7208 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.1302 - accuracy: 0.9737 - val_loss: 3.5882 - val_accuracy: 0.1111\n",
      "Epoch 81/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.1259 - accuracy: 0.9737 - val_loss: 3.6825 - val_accuracy: 0.1111\n",
      "Epoch 82/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.1221 - accuracy: 0.9868 - val_loss: 3.7201 - val_accuracy: 0.1111\n",
      "Epoch 83/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.1170 - accuracy: 0.9868 - val_loss: 3.7045 - val_accuracy: 0.1111\n",
      "Epoch 84/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.1147 - accuracy: 0.9737 - val_loss: 3.7252 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.1138 - accuracy: 0.9868 - val_loss: 3.7238 - val_accuracy: 0.1111\n",
      "Epoch 86/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.1067 - accuracy: 0.9868 - val_loss: 3.8351 - val_accuracy: 0.1111\n",
      "Epoch 87/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.1035 - accuracy: 0.9868 - val_loss: 3.7526 - val_accuracy: 0.1111\n",
      "Epoch 88/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.1030 - accuracy: 0.9868 - val_loss: 3.8447 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.1012 - accuracy: 0.9737 - val_loss: 3.8056 - val_accuracy: 0.1111\n",
      "Epoch 90/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0948 - accuracy: 1.0000 - val_loss: 3.7198 - val_accuracy: 0.1111\n",
      "Epoch 91/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 0.0953 - accuracy: 0.9737 - val_loss: 3.6949 - val_accuracy: 0.1111\n",
      "Epoch 92/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 0.0941 - accuracy: 0.9737 - val_loss: 3.6880 - val_accuracy: 0.1111\n",
      "Epoch 93/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0899 - accuracy: 0.9868 - val_loss: 3.7468 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0858 - accuracy: 0.9868 - val_loss: 3.7278 - val_accuracy: 0.1111\n",
      "Epoch 95/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0830 - accuracy: 0.9868 - val_loss: 3.7550 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0815 - accuracy: 0.9868 - val_loss: 3.7945 - val_accuracy: 0.1111\n",
      "Epoch 97/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 0.0825 - accuracy: 0.9737 - val_loss: 3.7149 - val_accuracy: 0.1111\n",
      "Epoch 98/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 0.0803 - accuracy: 0.9868 - val_loss: 3.7991 - val_accuracy: 0.1111\n",
      "Epoch 99/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0703 - accuracy: 1.0000 - val_loss: 3.8350 - val_accuracy: 0.1111\n",
      "Epoch 100/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0737 - accuracy: 0.9868 - val_loss: 3.9129 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0685 - accuracy: 0.9868 - val_loss: 3.8576 - val_accuracy: 0.1111\n",
      "Epoch 102/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0742 - accuracy: 0.9868 - val_loss: 3.8561 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0672 - accuracy: 0.9868 - val_loss: 3.8389 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 0.0657 - accuracy: 0.9868 - val_loss: 3.8364 - val_accuracy: 0.1111\n",
      "Epoch 105/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 0.0643 - accuracy: 0.9868 - val_loss: 3.8406 - val_accuracy: 0.1111\n",
      "Epoch 106/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 0.0638 - accuracy: 0.9868 - val_loss: 3.8942 - val_accuracy: 0.1111\n",
      "Epoch 107/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0637 - accuracy: 0.9868 - val_loss: 3.9047 - val_accuracy: 0.1111\n",
      "Epoch 108/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0629 - accuracy: 0.9868 - val_loss: 3.8216 - val_accuracy: 0.1111\n",
      "Epoch 109/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 0.0604 - accuracy: 0.9868 - val_loss: 3.9290 - val_accuracy: 0.1111\n",
      "Epoch 110/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0614 - accuracy: 0.9868 - val_loss: 3.8602 - val_accuracy: 0.1111\n",
      "Epoch 111/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0543 - accuracy: 1.0000 - val_loss: 3.9523 - val_accuracy: 0.1111\n",
      "Epoch 112/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0585 - accuracy: 0.9868 - val_loss: 3.9113 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0579 - accuracy: 0.9868 - val_loss: 3.8899 - val_accuracy: 0.1111\n",
      "Epoch 114/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0571 - accuracy: 0.9868 - val_loss: 3.9159 - val_accuracy: 0.1111\n",
      "Epoch 115/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0549 - accuracy: 0.9868 - val_loss: 3.9648 - val_accuracy: 0.1111\n",
      "Epoch 116/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0562 - accuracy: 0.9868 - val_loss: 4.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 0.0548 - accuracy: 0.9868 - val_loss: 3.9321 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0533 - accuracy: 0.9868 - val_loss: 3.9466 - val_accuracy: 0.1111\n",
      "Epoch 119/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0502 - accuracy: 0.9868 - val_loss: 3.9542 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0455 - accuracy: 1.0000 - val_loss: 3.9399 - val_accuracy: 0.1111\n",
      "Epoch 121/200\n",
      "76/76 [==============================] - 1s 10ms/sample - loss: 0.0510 - accuracy: 0.9868 - val_loss: 3.9691 - val_accuracy: 0.1111\n",
      "Epoch 122/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0485 - accuracy: 0.9868 - val_loss: 3.9139 - val_accuracy: 0.1111\n",
      "Epoch 123/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0493 - accuracy: 0.9868 - val_loss: 3.9890 - val_accuracy: 0.1111\n",
      "Epoch 124/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0488 - accuracy: 1.0000 - val_loss: 4.0029 - val_accuracy: 0.1111\n",
      "Epoch 125/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0473 - accuracy: 0.9868 - val_loss: 3.9659 - val_accuracy: 0.1111\n",
      "Epoch 126/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0469 - accuracy: 0.9868 - val_loss: 4.0132 - val_accuracy: 0.1111\n",
      "Epoch 127/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0460 - accuracy: 0.9868 - val_loss: 4.0626 - val_accuracy: 0.1111\n",
      "Epoch 128/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0429 - accuracy: 0.9868 - val_loss: 4.0531 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0451 - accuracy: 0.9868 - val_loss: 4.1141 - val_accuracy: 0.1111\n",
      "Epoch 130/200\n",
      "76/76 [==============================] - 1s 10ms/sample - loss: 0.0442 - accuracy: 0.9868 - val_loss: 4.0853 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0421 - accuracy: 0.9868 - val_loss: 4.1006 - val_accuracy: 0.1111\n",
      "Epoch 132/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0441 - accuracy: 0.9868 - val_loss: 4.0226 - val_accuracy: 0.1111\n",
      "Epoch 133/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0422 - accuracy: 0.9868 - val_loss: 4.0406 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0418 - accuracy: 0.9868 - val_loss: 4.0889 - val_accuracy: 0.1111\n",
      "Epoch 135/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0409 - accuracy: 1.0000 - val_loss: 4.1261 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0421 - accuracy: 0.9737 - val_loss: 4.1187 - val_accuracy: 0.1111\n",
      "Epoch 137/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0396 - accuracy: 0.9868 - val_loss: 4.0159 - val_accuracy: 0.1111\n",
      "Epoch 138/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0384 - accuracy: 0.9868 - val_loss: 4.1364 - val_accuracy: 0.1111\n",
      "Epoch 139/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0394 - accuracy: 0.9868 - val_loss: 4.0898 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0374 - accuracy: 0.9868 - val_loss: 4.0977 - val_accuracy: 0.1111\n",
      "Epoch 141/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0369 - accuracy: 0.9868 - val_loss: 4.2034 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/200\n",
      "76/76 [==============================] - 1s 10ms/sample - loss: 0.0377 - accuracy: 0.9868 - val_loss: 4.1718 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0338 - accuracy: 0.9868 - val_loss: 4.1250 - val_accuracy: 0.1111\n",
      "Epoch 144/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0333 - accuracy: 1.0000 - val_loss: 4.1392 - val_accuracy: 0.1111\n",
      "Epoch 145/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0334 - accuracy: 1.0000 - val_loss: 4.1717 - val_accuracy: 0.1111\n",
      "Epoch 146/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0334 - accuracy: 1.0000 - val_loss: 4.0861 - val_accuracy: 0.1111\n",
      "Epoch 147/200\n",
      "76/76 [==============================] - 1s 10ms/sample - loss: 0.0340 - accuracy: 0.9868 - val_loss: 4.1201 - val_accuracy: 0.1111\n",
      "Epoch 148/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0339 - accuracy: 0.9868 - val_loss: 4.1263 - val_accuracy: 0.1111\n",
      "Epoch 149/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0342 - accuracy: 0.9868 - val_loss: 4.2344 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0340 - accuracy: 1.0000 - val_loss: 4.2255 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0346 - accuracy: 0.9868 - val_loss: 4.1211 - val_accuracy: 0.1111\n",
      "Epoch 152/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0339 - accuracy: 0.9868 - val_loss: 4.1365 - val_accuracy: 0.1111\n",
      "Epoch 153/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0346 - accuracy: 0.9868 - val_loss: 4.0961 - val_accuracy: 0.1111\n",
      "Epoch 154/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0333 - accuracy: 1.0000 - val_loss: 4.1871 - val_accuracy: 0.1111\n",
      "Epoch 155/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0334 - accuracy: 1.0000 - val_loss: 4.2303 - val_accuracy: 0.1111\n",
      "Epoch 156/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0326 - accuracy: 0.9868 - val_loss: 4.1843 - val_accuracy: 0.1111\n",
      "Epoch 157/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0323 - accuracy: 0.9868 - val_loss: 4.2146 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0327 - accuracy: 1.0000 - val_loss: 4.2603 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/200\n",
      "76/76 [==============================] - 1s 10ms/sample - loss: 0.0333 - accuracy: 1.0000 - val_loss: 4.1554 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0320 - accuracy: 1.0000 - val_loss: 4.2110 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0316 - accuracy: 1.0000 - val_loss: 4.3400 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0328 - accuracy: 0.9868 - val_loss: 4.2420 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/200\n",
      "76/76 [==============================] - 1s 10ms/sample - loss: 0.0281 - accuracy: 1.0000 - val_loss: 4.2064 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/200\n",
      "76/76 [==============================] - 1s 10ms/sample - loss: 0.0274 - accuracy: 1.0000 - val_loss: 4.2246 - val_accuracy: 0.1111\n",
      "Epoch 165/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0280 - accuracy: 1.0000 - val_loss: 4.2703 - val_accuracy: 0.1111\n",
      "Epoch 166/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0286 - accuracy: 1.0000 - val_loss: 4.2056 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0246 - accuracy: 1.0000 - val_loss: 4.2203 - val_accuracy: 0.1111\n",
      "Epoch 168/200\n",
      "76/76 [==============================] - 1s 10ms/sample - loss: 0.0288 - accuracy: 0.9868 - val_loss: 4.2227 - val_accuracy: 0.1111\n",
      "Epoch 169/200\n",
      "76/76 [==============================] - 1s 10ms/sample - loss: 0.0268 - accuracy: 0.9868 - val_loss: 4.2278 - val_accuracy: 0.1111\n",
      "Epoch 170/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0253 - accuracy: 1.0000 - val_loss: 4.2715 - val_accuracy: 0.1111\n",
      "Epoch 171/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0252 - accuracy: 1.0000 - val_loss: 4.2654 - val_accuracy: 0.1111\n",
      "Epoch 172/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0258 - accuracy: 1.0000 - val_loss: 4.2526 - val_accuracy: 0.1111\n",
      "Epoch 173/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0229 - accuracy: 1.0000 - val_loss: 4.2710 - val_accuracy: 0.1111\n",
      "Epoch 174/200\n",
      "76/76 [==============================] - 1s 10ms/sample - loss: 0.0268 - accuracy: 0.9868 - val_loss: 4.2876 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0276 - accuracy: 1.0000 - val_loss: 4.2882 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0277 - accuracy: 1.0000 - val_loss: 4.3016 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0263 - accuracy: 0.9868 - val_loss: 4.2898 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0275 - accuracy: 0.9868 - val_loss: 4.2949 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0271 - accuracy: 1.0000 - val_loss: 4.3284 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0259 - accuracy: 0.9868 - val_loss: 4.3085 - val_accuracy: 0.1111\n",
      "Epoch 181/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0259 - accuracy: 0.9868 - val_loss: 4.3450 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0252 - accuracy: 0.9868 - val_loss: 4.3108 - val_accuracy: 0.1111\n",
      "Epoch 183/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0262 - accuracy: 0.9868 - val_loss: 4.3382 - val_accuracy: 0.1111\n",
      "Epoch 184/200\n",
      "76/76 [==============================] - 1s 10ms/sample - loss: 0.0267 - accuracy: 1.0000 - val_loss: 4.3285 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0249 - accuracy: 0.9868 - val_loss: 4.2885 - val_accuracy: 0.1111\n",
      "Epoch 186/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0254 - accuracy: 0.9868 - val_loss: 4.2623 - val_accuracy: 0.1111\n",
      "Epoch 187/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0231 - accuracy: 1.0000 - val_loss: 4.2684 - val_accuracy: 0.1111\n",
      "Epoch 188/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0224 - accuracy: 1.0000 - val_loss: 4.3183 - val_accuracy: 0.1111\n",
      "Epoch 189/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0224 - accuracy: 1.0000 - val_loss: 4.2910 - val_accuracy: 0.1111\n",
      "Epoch 190/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0234 - accuracy: 1.0000 - val_loss: 4.3346 - val_accuracy: 0.1111\n",
      "Epoch 191/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0233 - accuracy: 1.0000 - val_loss: 4.3230 - val_accuracy: 0.1111\n",
      "Epoch 192/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0229 - accuracy: 1.0000 - val_loss: 4.3729 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0237 - accuracy: 0.9868 - val_loss: 4.3344 - val_accuracy: 0.1111\n",
      "Epoch 194/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0245 - accuracy: 0.9868 - val_loss: 4.3692 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/200\n",
      "76/76 [==============================] - 1s 10ms/sample - loss: 0.0237 - accuracy: 0.9868 - val_loss: 4.3536 - val_accuracy: 0.1111\n",
      "Epoch 196/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0241 - accuracy: 1.0000 - val_loss: 4.3607 - val_accuracy: 0.1111\n",
      "Epoch 197/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0243 - accuracy: 1.0000 - val_loss: 4.3316 - val_accuracy: 0.1111\n",
      "Epoch 198/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0250 - accuracy: 1.0000 - val_loss: 4.3442 - val_accuracy: 0.1111\n",
      "Epoch 199/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 0.0244 - accuracy: 1.0000 - val_loss: 4.3187 - val_accuracy: 0.1111\n",
      "Epoch 200/200\n",
      "76/76 [==============================] - 1s 9ms/sample - loss: 0.0235 - accuracy: 0.9868 - val_loss: 4.3461 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_x_t, y_train,validation_split=0.1, batch_size=5, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85,)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_39 (Conv2D)           (None, 148, 148, 32)      43232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 148, 148, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 148, 148, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 148, 148, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 74, 74, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 74, 74, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 74, 74, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 74, 74, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 37, 37, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 82944)             0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 256)               21233920  \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 28)                3612      \n",
      "=================================================================\n",
      "Total params: 21,713,500\n",
      "Trainable params: 21,711,964\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.regularizers import l2\n",
    "model_1 = Sequential()\n",
    "model_1.add(Conv2D(32, kernel_size=(3, 3), activation='relu', \n",
    "                 input_shape=(150,150,150)))\n",
    "model_1.add(BatchNormalization())\n",
    "model_1.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "model_1.add(BatchNormalization())\n",
    "model_1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_1.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "model_1.add(BatchNormalization())\n",
    "model_1.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "model_1.add(BatchNormalization())\n",
    "model_1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_1.add(Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "model_1.add(BatchNormalization())\n",
    "model_1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_1.add(Flatten())\n",
    "# Densely connected layers\n",
    "model_1.add(Dense(256, activation='relu',kernel_regularizer=keras.regularizers.l2(0.001), bias_regularizer=l2(0.001)))\n",
    "model_1.add(BatchNormalization())\n",
    "model_1.add(Dropout(.5))\n",
    "model_1.add(Dense(128, activation='relu',kernel_regularizer=keras.regularizers.l2(0.001), bias_regularizer=l2(0.001)))\n",
    "model_1.add(Dropout(.25))\n",
    "model_1.add(Dense(28, activation='softmax'))\n",
    "\n",
    "model_1.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 76 samples, validate on 9 samples\n",
      "Epoch 1/200\n",
      "76/76 [==============================] - 2s 23ms/sample - loss: 4.9736 - accuracy: 0.0263 - val_loss: 4.0065 - val_accuracy: 0.1111\n",
      "Epoch 2/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 4.7772 - accuracy: 0.0263 - val_loss: 4.0220 - val_accuracy: 0.1111\n",
      "Epoch 3/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 5.0656 - accuracy: 0.0395 - val_loss: 4.0404 - val_accuracy: 0.1111\n",
      "Epoch 4/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 4.8141 - accuracy: 0.0526 - val_loss: 4.0523 - val_accuracy: 0.1111\n",
      "Epoch 5/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 4.7160 - accuracy: 0.0395 - val_loss: 4.0664 - val_accuracy: 0.1111\n",
      "Epoch 6/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 4.6332 - accuracy: 0.0789 - val_loss: 4.0710 - val_accuracy: 0.1111\n",
      "Epoch 7/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 4.8656 - accuracy: 0.0263 - val_loss: 4.0520 - val_accuracy: 0.1111\n",
      "Epoch 8/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 4.5342 - accuracy: 0.0263 - val_loss: 4.0251 - val_accuracy: 0.1111\n",
      "Epoch 9/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 4.6851 - accuracy: 0.1053 - val_loss: 3.9920 - val_accuracy: 0.1111\n",
      "Epoch 10/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 4.4427 - accuracy: 0.0263 - val_loss: 3.9616 - val_accuracy: 0.1111\n",
      "Epoch 11/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 4.5082 - accuracy: 0.0921 - val_loss: 3.9353 - val_accuracy: 0.1111\n",
      "Epoch 12/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 4.4054 - accuracy: 0.0658 - val_loss: 3.9132 - val_accuracy: 0.1111\n",
      "Epoch 13/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 4.1567 - accuracy: 0.0526 - val_loss: 3.8864 - val_accuracy: 0.1111\n",
      "Epoch 14/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 4.3314 - accuracy: 0.0658 - val_loss: 3.8838 - val_accuracy: 0.1111\n",
      "Epoch 15/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 3.9793 - accuracy: 0.1053 - val_loss: 3.8827 - val_accuracy: 0.1111\n",
      "Epoch 16/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 4.2785 - accuracy: 0.0789 - val_loss: 3.9037 - val_accuracy: 0.1111\n",
      "Epoch 17/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 4.3300 - accuracy: 0.1184 - val_loss: 3.9461 - val_accuracy: 0.1111\n",
      "Epoch 18/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 4.1941 - accuracy: 0.1053 - val_loss: 3.9866 - val_accuracy: 0.1111\n",
      "Epoch 19/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 4.2142 - accuracy: 0.1053 - val_loss: 4.0452 - val_accuracy: 0.1111\n",
      "Epoch 20/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.8876 - accuracy: 0.1842 - val_loss: 4.1285 - val_accuracy: 0.1111\n",
      "Epoch 21/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.9361 - accuracy: 0.1184 - val_loss: 4.2077 - val_accuracy: 0.1111\n",
      "Epoch 22/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 4.1559 - accuracy: 0.0921 - val_loss: 4.2979 - val_accuracy: 0.1111\n",
      "Epoch 23/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.8609 - accuracy: 0.1711 - val_loss: 4.3813 - val_accuracy: 0.1111\n",
      "Epoch 24/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 4.1469 - accuracy: 0.0789 - val_loss: 4.4665 - val_accuracy: 0.1111\n",
      "Epoch 25/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 4.1747 - accuracy: 0.1316 - val_loss: 4.5739 - val_accuracy: 0.1111\n",
      "Epoch 26/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.9862 - accuracy: 0.0526 - val_loss: 4.6365 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 4.0629 - accuracy: 0.0526 - val_loss: 4.6783 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.9585 - accuracy: 0.0789 - val_loss: 4.6824 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.9815 - accuracy: 0.1184 - val_loss: 4.6840 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.9672 - accuracy: 0.1711 - val_loss: 4.6812 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.9129 - accuracy: 0.1842 - val_loss: 4.6535 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.9809 - accuracy: 0.1447 - val_loss: 4.6664 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 3.7534 - accuracy: 0.1316 - val_loss: 4.6734 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.8017 - accuracy: 0.1316 - val_loss: 4.6842 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.7524 - accuracy: 0.1842 - val_loss: 4.6839 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.7298 - accuracy: 0.1447 - val_loss: 4.6858 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.6742 - accuracy: 0.1711 - val_loss: 4.6890 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.5452 - accuracy: 0.1974 - val_loss: 4.6995 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.7736 - accuracy: 0.1711 - val_loss: 4.7110 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 3.5646 - accuracy: 0.1711 - val_loss: 4.6936 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 3.4127 - accuracy: 0.2237 - val_loss: 4.6826 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.4396 - accuracy: 0.1974 - val_loss: 4.6721 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 3.6355 - accuracy: 0.2105 - val_loss: 4.6510 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.4230 - accuracy: 0.2500 - val_loss: 4.5959 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.4895 - accuracy: 0.2368 - val_loss: 4.5707 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.5480 - accuracy: 0.2237 - val_loss: 4.5342 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.3735 - accuracy: 0.2763 - val_loss: 4.4885 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.4303 - accuracy: 0.2105 - val_loss: 4.4584 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.2556 - accuracy: 0.2105 - val_loss: 4.4524 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.2917 - accuracy: 0.2368 - val_loss: 4.4663 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.2527 - accuracy: 0.2500 - val_loss: 4.4819 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.2364 - accuracy: 0.2895 - val_loss: 4.4855 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.2418 - accuracy: 0.2895 - val_loss: 4.5020 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.1888 - accuracy: 0.2895 - val_loss: 4.4889 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.3722 - accuracy: 0.2368 - val_loss: 4.4884 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.1583 - accuracy: 0.2632 - val_loss: 4.4813 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.3680 - accuracy: 0.1974 - val_loss: 4.4725 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 3.1598 - accuracy: 0.2632 - val_loss: 4.4571 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.9490 - accuracy: 0.3158 - val_loss: 4.4474 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 2.8523 - accuracy: 0.3684 - val_loss: 4.4601 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.2591 - accuracy: 0.2763 - val_loss: 4.4824 - val_accuracy: 0.1111\n",
      "Epoch 62/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.9236 - accuracy: 0.3684 - val_loss: 4.4734 - val_accuracy: 0.1111\n",
      "Epoch 63/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.1126 - accuracy: 0.2105 - val_loss: 4.4507 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 3.1018 - accuracy: 0.3158 - val_loss: 4.4856 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 2.9611 - accuracy: 0.3421 - val_loss: 4.4630 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.2348 - accuracy: 0.2368 - val_loss: 4.4744 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.0843 - accuracy: 0.2763 - val_loss: 4.4647 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 2.7124 - accuracy: 0.4079 - val_loss: 4.4552 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.7320 - accuracy: 0.4605 - val_loss: 4.4919 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.1347 - accuracy: 0.3684 - val_loss: 4.5181 - val_accuracy: 0.1111\n",
      "Epoch 71/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.9671 - accuracy: 0.3289 - val_loss: 4.5030 - val_accuracy: 0.1111\n",
      "Epoch 72/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.0424 - accuracy: 0.3947 - val_loss: 4.4943 - val_accuracy: 0.1111\n",
      "Epoch 73/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 2.8946 - accuracy: 0.3289 - val_loss: 4.4915 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 2.9865 - accuracy: 0.3421 - val_loss: 4.5083 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.7521 - accuracy: 0.4079 - val_loss: 4.4893 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.8843 - accuracy: 0.3947 - val_loss: 4.4886 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.8258 - accuracy: 0.3158 - val_loss: 4.4924 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 3.0640 - accuracy: 0.2632 - val_loss: 4.4913 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.9103 - accuracy: 0.2763 - val_loss: 4.5067 - val_accuracy: 0.1111\n",
      "Epoch 80/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.8698 - accuracy: 0.3158 - val_loss: 4.5150 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 2.9161 - accuracy: 0.3289 - val_loss: 4.4999 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.6173 - accuracy: 0.4474 - val_loss: 4.4818 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.7018 - accuracy: 0.3684 - val_loss: 4.4798 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.9270 - accuracy: 0.3158 - val_loss: 4.4621 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.4867 - accuracy: 0.5263 - val_loss: 4.4770 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.8356 - accuracy: 0.3684 - val_loss: 4.4767 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.4386 - accuracy: 0.4737 - val_loss: 4.4847 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.5450 - accuracy: 0.4211 - val_loss: 4.5086 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.7517 - accuracy: 0.3947 - val_loss: 4.5053 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.5452 - accuracy: 0.5132 - val_loss: 4.5115 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.5901 - accuracy: 0.4342 - val_loss: 4.4887 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.7722 - accuracy: 0.3947 - val_loss: 4.4692 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.6486 - accuracy: 0.4211 - val_loss: 4.4880 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.6160 - accuracy: 0.4737 - val_loss: 4.5086 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.7741 - accuracy: 0.4079 - val_loss: 4.5037 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.4207 - accuracy: 0.5263 - val_loss: 4.5184 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.4370 - accuracy: 0.5132 - val_loss: 4.5381 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.4690 - accuracy: 0.4474 - val_loss: 4.5263 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 2.5854 - accuracy: 0.4342 - val_loss: 4.5278 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.3403 - accuracy: 0.5526 - val_loss: 4.5379 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.2656 - accuracy: 0.5263 - val_loss: 4.5503 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.5648 - accuracy: 0.3947 - val_loss: 4.5473 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.4585 - accuracy: 0.52 - 1s 11ms/sample - loss: 2.4790 - accuracy: 0.5132 - val_loss: 4.5376 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.4914 - accuracy: 0.4605 - val_loss: 4.4910 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.4744 - accuracy: 0.4737 - val_loss: 4.4645 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.6930 - accuracy: 0.3816 - val_loss: 4.4544 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.4103 - accuracy: 0.5263 - val_loss: 4.4672 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.5715 - accuracy: 0.4868 - val_loss: 4.4918 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 2.4511 - accuracy: 0.5263 - val_loss: 4.5268 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.6601 - accuracy: 0.3816 - val_loss: 4.5536 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.3314 - accuracy: 0.4868 - val_loss: 4.5488 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.5224 - accuracy: 0.4605 - val_loss: 4.5550 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 2.2845 - accuracy: 0.5526 - val_loss: 4.5902 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.2838 - accuracy: 0.5921 - val_loss: 4.5746 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 2.3226 - accuracy: 0.5658 - val_loss: 4.5776 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 2.4325 - accuracy: 0.4868 - val_loss: 4.5516 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 2.5404 - accuracy: 0.5000 - val_loss: 4.5362 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 2.4644 - accuracy: 0.5132 - val_loss: 4.5179 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.3079 - accuracy: 0.5395 - val_loss: 4.5004 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.4196 - accuracy: 0.5395 - val_loss: 4.5156 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.1746 - accuracy: 0.6053 - val_loss: 4.5328 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.2343 - accuracy: 0.5263 - val_loss: 4.5463 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.4349 - accuracy: 0.5263 - val_loss: 4.5515 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.2400 - accuracy: 0.5789 - val_loss: 4.5571 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 2.2382 - accuracy: 0.5526 - val_loss: 4.5459 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.1375 - accuracy: 0.5658 - val_loss: 4.5310 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.2352 - accuracy: 0.5789 - val_loss: 4.5447 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.4987 - accuracy: 0.5263 - val_loss: 4.5510 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.2190 - accuracy: 0.5658 - val_loss: 4.5477 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.2046 - accuracy: 0.5263 - val_loss: 4.5379 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.2826 - accuracy: 0.5395 - val_loss: 4.5153 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.3953 - accuracy: 0.4737 - val_loss: 4.5262 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 2.1903 - accuracy: 0.4737 - val_loss: 4.5059 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.0669 - accuracy: 0.6842 - val_loss: 4.5179 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 2.1556 - accuracy: 0.6184 - val_loss: 4.5381 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.0985 - accuracy: 0.5658 - val_loss: 4.5557 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.1373 - accuracy: 0.6711 - val_loss: 4.5467 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.3675 - accuracy: 0.5526 - val_loss: 4.5480 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.1206 - accuracy: 0.5921 - val_loss: 4.5328 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.2009 - accuracy: 0.5789 - val_loss: 4.5371 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.1604 - accuracy: 0.6842 - val_loss: 4.5354 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.0651 - accuracy: 0.6184 - val_loss: 4.5210 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 2.2714 - accuracy: 0.5395 - val_loss: 4.5473 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.1940 - accuracy: 0.5658 - val_loss: 4.5657 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.1996 - accuracy: 0.5921 - val_loss: 4.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.1772 - accuracy: 0.6053 - val_loss: 4.5668 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.1413 - accuracy: 0.6842 - val_loss: 4.5617 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.1876 - accuracy: 0.5526 - val_loss: 4.5750 - val_accuracy: 0.1111\n",
      "Epoch 149/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.0952 - accuracy: 0.6316 - val_loss: 4.5900 - val_accuracy: 0.1111\n",
      "Epoch 150/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.9856 - accuracy: 0.7368 - val_loss: 4.5982 - val_accuracy: 0.1111\n",
      "Epoch 151/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 2.1354 - accuracy: 0.5921 - val_loss: 4.6054 - val_accuracy: 0.1111\n",
      "Epoch 152/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.1701 - accuracy: 0.5921 - val_loss: 4.5944 - val_accuracy: 0.1111\n",
      "Epoch 153/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.9704 - accuracy: 0.6974 - val_loss: 4.6013 - val_accuracy: 0.1111\n",
      "Epoch 154/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.1223 - accuracy: 0.6053 - val_loss: 4.6079 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.1731 - accuracy: 0.5921 - val_loss: 4.5901 - val_accuracy: 0.1111\n",
      "Epoch 156/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.9799 - accuracy: 0.7237 - val_loss: 4.6011 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.0647 - accuracy: 0.5921 - val_loss: 4.5890 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.0308 - accuracy: 0.6053 - val_loss: 4.6088 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.0832 - accuracy: 0.6316 - val_loss: 4.6025 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 1.8939 - accuracy: 0.6974 - val_loss: 4.6255 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.1480 - accuracy: 0.6053 - val_loss: 4.6136 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 2.1549 - accuracy: 0.60 - 1s 12ms/sample - loss: 2.1794 - accuracy: 0.5921 - val_loss: 4.6148 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.0551 - accuracy: 0.6316 - val_loss: 4.6018 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.1760 - accuracy: 0.6053 - val_loss: 4.6281 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.8956 - accuracy: 0.6974 - val_loss: 4.6240 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.9921 - accuracy: 0.6842 - val_loss: 4.5908 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 2.0111 - accuracy: 0.6316 - val_loss: 4.5823 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.9024 - accuracy: 0.6974 - val_loss: 4.5765 - val_accuracy: 0.1111\n",
      "Epoch 169/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.9292 - accuracy: 0.6579 - val_loss: 4.5567 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.0100 - accuracy: 0.6974 - val_loss: 4.5569 - val_accuracy: 0.1111\n",
      "Epoch 171/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.9950 - accuracy: 0.6842 - val_loss: 4.5765 - val_accuracy: 0.1111\n",
      "Epoch 172/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.1176 - accuracy: 0.6184 - val_loss: 4.5700 - val_accuracy: 0.1111\n",
      "Epoch 173/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 2.1757 - accuracy: 0.6316 - val_loss: 4.5554 - val_accuracy: 0.1111\n",
      "Epoch 174/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.8702 - accuracy: 0.7368 - val_loss: 4.5796 - val_accuracy: 0.1111\n",
      "Epoch 175/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 1.9994 - accuracy: 0.6184 - val_loss: 4.5773 - val_accuracy: 0.1111\n",
      "Epoch 176/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.8582 - accuracy: 0.7368 - val_loss: 4.5703 - val_accuracy: 0.1111\n",
      "Epoch 177/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.9180 - accuracy: 0.6579 - val_loss: 4.5568 - val_accuracy: 0.1111\n",
      "Epoch 178/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.9065 - accuracy: 0.6842 - val_loss: 4.5673 - val_accuracy: 0.1111\n",
      "Epoch 179/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.9072 - accuracy: 0.7632 - val_loss: 4.5643 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.9654 - accuracy: 0.6974 - val_loss: 4.5495 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.8306 - accuracy: 0.6711 - val_loss: 4.5654 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.8888 - accuracy: 0.7237 - val_loss: 4.5712 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.9847 - accuracy: 0.6316 - val_loss: 4.5633 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.9307 - accuracy: 0.6711 - val_loss: 4.5615 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.8807 - accuracy: 0.7368 - val_loss: 4.5584 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.8395 - accuracy: 0.7763 - val_loss: 4.5498 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.9579 - accuracy: 0.7105 - val_loss: 4.5657 - val_accuracy: 0.1111\n",
      "Epoch 188/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.8069 - accuracy: 0.7368 - val_loss: 4.5514 - val_accuracy: 0.1111\n",
      "Epoch 189/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.8340 - accuracy: 0.6711 - val_loss: 4.5566 - val_accuracy: 0.1111\n",
      "Epoch 190/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.8527 - accuracy: 0.6974 - val_loss: 4.5482 - val_accuracy: 0.1111\n",
      "Epoch 191/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.8533 - accuracy: 0.7105 - val_loss: 4.5493 - val_accuracy: 0.1111\n",
      "Epoch 192/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.8272 - accuracy: 0.7368 - val_loss: 4.5439 - val_accuracy: 0.1111\n",
      "Epoch 193/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.7702 - accuracy: 0.7895 - val_loss: 4.5220 - val_accuracy: 0.1111\n",
      "Epoch 194/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.8172 - accuracy: 0.7237 - val_loss: 4.4977 - val_accuracy: 0.1111\n",
      "Epoch 195/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.9975 - accuracy: 0.6842 - val_loss: 4.5191 - val_accuracy: 0.1111\n",
      "Epoch 196/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.9033 - accuracy: 0.6842 - val_loss: 4.5355 - val_accuracy: 0.1111\n",
      "Epoch 197/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.8502 - accuracy: 0.6974 - val_loss: 4.5645 - val_accuracy: 0.1111\n",
      "Epoch 198/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 1.8899 - accuracy: 0.74 - 1s 12ms/sample - loss: 1.9179 - accuracy: 0.7368 - val_loss: 4.5895 - val_accuracy: 0.1111\n",
      "Epoch 199/200\n",
      "76/76 [==============================] - 1s 12ms/sample - loss: 1.8508 - accuracy: 0.7237 - val_loss: 4.6080 - val_accuracy: 0.1111\n",
      "Epoch 200/200\n",
      "76/76 [==============================] - 1s 11ms/sample - loss: 1.7845 - accuracy: 0.7368 - val_loss: 4.6128 - val_accuracy: 0.1111\n"
     ]
    }
   ],
   "source": [
    "history = model_1.fit(train_x_t, y_train,validation_split=0.1, batch_size=5, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = [np.array(z) for p in test_x]\n",
    "test_x_t = np.array(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14,)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = [np.array(x) for x in train_x]\n",
    "train_x_t = np.array(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 1s 15ms/sample - loss: 1.2284 - accuracy: 0.9059\n",
      "Train loss: 1.2284007212694954\n",
      "Train accuracy: 0.90588236\n"
     ]
    }
   ],
   "source": [
    "# score = model_1.evaluate(test_x_t, y_test, verbose = 1) \n",
    "\n",
    "# print('Test loss:', score[0]) \n",
    "# print('Test accuracy:', score[1])\n",
    "score = model_1.evaluate(train_x_t, y_train, verbose = 1) \n",
    "\n",
    "print('Train loss:', score[0]) \n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = [np.array(p) for p in test_x]\n",
    "test_x_t = np.array(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14,)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14,)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 150, 150, 150)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

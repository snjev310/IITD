{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b27676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization, MaxPool2D\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d597772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abdfa525",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image = []\n",
    "all_label = []\n",
    "def vol_(path):\n",
    "    for index,folder in enumerate(os.listdir(path)):\n",
    "        print(index,folder)\n",
    "        image = []\n",
    "        for row in os.listdir(path+'\\\\'+folder):\n",
    "            pat_path = path+'\\\\'+folder+'\\\\'+row\n",
    "            all_image.append(pat_path)\n",
    "            all_label.append(index)\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97b6e69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 00_5\n",
      "1 06_10\n",
      "2 11_16\n",
      "3 17_28\n"
     ]
    }
   ],
   "source": [
    "image_train_set = vol_(r'C:\\Users\\AIIMS-IITD\\Desktop\\Sanjeev\\4_class_cluster_volume\\Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a0bb032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_maps(path,label):\n",
    "    #print(path[0])\n",
    "    images = []\n",
    "    for index,item in enumerate(path):\n",
    "        for name in enumerate(os.listdir(item)):\n",
    "            #print(name[1])\n",
    "            folder = os.path.join(item, name[1])\n",
    "            #print('folder: ',folder)\n",
    "            img = cv2.imread(folder)\n",
    "            #print(img.shape)\n",
    "            if img is not None:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "                img1 = cv2.resize(img, (196, 196))\n",
    "                img = np.dstack((img1,img1,img1))\n",
    "            if img is not None:\n",
    "           #     img = (img-np.mean(img))/np.std(img)\n",
    "                images.append((np.array(img),label[index]))\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1959756",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(input_shape=(196,196,3),filters=64,kernel_size=(5,5),padding='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(2,2))\n",
    "\n",
    "model.add(Conv2D(filters=32,kernel_size=(3,3),padding='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters=16,kernel_size=(3,3),padding='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(MaxPooling2D(2,2))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten(name='flatten'))\n",
    "#model.add(Dense(16, activation='relu', name='fc1'))\n",
    "model.add(Dense(4, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "232094c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "960/960 [==============================] - 23s 21ms/step - loss: 2.8122 - accuracy: 0.3114 - auc: 0.5687: 19s - loss: 3.2618 - accuracy: 0.2267 - auc: 0. - ETA: 19s - loss: 3.1960 - accuracy:  - ETA: 18s - loss: 3.1071 - accuracy: 0.2565 - auc: 0.50 - ETA: 18s - loss: 3.1017 - accuracy: 0.2583 - auc: 0 - ETA: 17s - loss: 3.1017 - accuracy: 0.2541 - auc: 0.50 - ETA: 17s - loss: 3.0859 - accuracy: 0.2579 - auc: 0 - ETA: 17s - loss: 3.0749 - accuracy: 0.2592 - auc: 0.5 - ETA: 17s - loss: 3.0735 - accuracy: 0.2583 - auc: 0. - ETA: 16s - loss: 3.0721 - accuracy: 0 - ETA: 15s - loss: 3.0204 - accura - ETA: 13s - loss: 2.9731 - accuracy: 0.2791 - auc: 0.52 - ETA: 13s - loss: 2.9685 - accuracy: 0 - ETA: 12s - loss: 2.9460 - accuracy: 0.2868 - auc: - ETA: 12s - loss: 2.9326 - accuracy: 0.2904 -  - ETA: 11s - loss: 2.9243 - accuracy: 0.2919 - au - ETA: 10s - loss: 2.9148 - accuracy: 0.2924 - auc: 0.542 - ETA: 10s - loss: 2.9132 - accuracy: 0.2927 - auc: 0. - ETA: 10s - loss: 2.9051 - accuracy: 0.2939 - auc: 0.54 - ETA: 10s - loss: 2.9009 - accuracy: 0.2940 - auc: 0.5 - ETA: 10s - loss: 2.9014 - accuracy: 0.2942 - ETA: 9s - loss: 2.8970 - accuracy: 0.2943 - auc: 0. - ETA: 9s - loss: 2.8951 - accuracy: 0.2955 - auc: 0. - ETA:  - ETA: 7s - loss: 2.8758 - accuracy: 0.2991 - auc:  - ETA: 7s - loss: 2.8748 - accuracy: 0.2986 - auc: 0.55 - ETA: 7s - l - ETA: 6s - loss: 2.8566 - accuracy: 0.3029 - auc: 0.55 - ETA: 6s - loss: 2.8560 - accuracy: 0.3032 - auc: 0.55 - ETA: 5s - loss: 2.8541 - accuracy: 0.3036 - a - ETA: 5s - loss: 2.8475 - accuracy: 0.3048 - auc:  - ETA: 5s - loss: 2.8470 - accuracy - ETA: 4s - loss: 2.8389 - accuracy: 0.3076 - auc: 0.56 - ETA: 4s - loss: 2.8399 - accuracy: 0.3075 - auc - ETA: 4s - loss: 2.839 - ETA: 3s - loss: 2.8400 - accuracy: 0.3070 - auc - ETA: 3s - loss: 2.8356 - accuracy - ETA: 2s - loss: 2.8258 - ac - ETA: 1s - loss: 2.8185 - accuracy:  - ETA: 0s - loss: 2.8199 - accuracy: 0.3102 - auc: 0. - ETA: 0s - loss: 2.8197 - accuracy: 0.3098 - ETA: 0s - loss: 2.8144 - accuracy: 0.3107 - a - ETA: 0s - loss: 2.8114 - accuracy: 0.3115 - auc: 0.\n",
      "Epoch 2/15\n",
      "960/960 [==============================] - 20s 21ms/step - loss: 2.5571 - accuracy: 0.3649 - auc: 0.6291: 19s - loss: 2.6237 - accuracy: 0.3438 - auc: 0 - ETA: 19s - loss: 2.6469 - a - ETA: 13s - loss: 2.6317 - a - ETA: 12s - loss: 2.6048 - accuracy: 0.3472 - auc: 0.61 - ETA: 11s - loss: 2.6060 - accuracy: 0.3476 - ETA: 11s - loss: 2.6118 - - ETA: 0s - loss: 2.5582 - accuracy: 0.3650 - auc: 0. - ETA: 0s - loss: 2.5574 - accuracy: 0.3652 - auc: \n",
      "Epoch 3/15\n",
      "960/960 [==============================] - 20s 21ms/step - loss: 2.3673 - accuracy: 0.4078 - auc: 0.6665: 18s - l - ETA: 12s - loss: 2.4287 - accuracy: 0.3975 - auc: 0.654 - ETA: 12s - loss: 2.4308 - accuracy: 0.3970 - - ETA: 11s - loss: 2.4325 - ETA: 9s - loss: - ETA: 4s - loss: 2.3897 - accuracy: 0.4028 - - ETA: 4s - loss: 2.3884 - accuracy: 0. - ETA: 3s - loss: 2.3891 - accuracy: 0.40 - ETA: 3s - loss: 2.3875 - accuracy: 0.4031 - ETA: 2s - loss: 2.3876 - accuracy: 0.4024 - auc: 0.66 - ETA: 2s - loss: 2.385 - ETA: 1s - loss: 2.3778 -  - ETA: 0s - loss: 2.3692 - accuracy: 0. - ETA: 0s - loss: 2.3669 - accuracy: 0.4080 - auc: 0.66 - ETA: 0s - loss: 2.3674 - accuracy: 0.4078 - a\n",
      "Epoch 4/15\n",
      "960/960 [==============================] - 20s 21ms/step - loss: 2.2128 - accuracy: 0.4542 - auc: 0.6996: 16s - loss: 2.2845 - accur - ETA: 15s - loss: 2.2743 - accuracy: 0.440 - ETA: 14s - loss: 2.2586 - accuracy: 0.4465 - auc: 0.687 - ETA: 14s - loss: 2.2567 - accuracy: 0.4471 - auc: 0.68 - ETA: 14s - loss: 2.2546 - acc - ETA: 12s - los - ETA: 10s - l - ETA: 8s - loss: 2.2409 - accuracy: 0.4471 - auc: 0.69 - ETA: 8s - loss: 2.2406 - accuracy: 0.4470 - auc: 0. - ETA: 8s - loss: 2.2406 - accuracy:  - ETA: 7s - loss: 2.2361 - accuracy: 0.4474 - ETA: 7s - loss: 2.2378 - accuracy: 0.4471 - auc - ETA:  - ETA: 5s - loss: 2.2286 - accu - ETA: 4s - loss: 2.2231 - accuracy: 0.4508 - auc - ETA: 1s - loss: 2.2162 - accuracy: 0. - ETA: 0s - loss: 2.2123 - accuracy: 0.4541 - auc: 0. - ETA: 0s - loss: 2.2124 - accuracy: 0.4542\n",
      "Epoch 5/15\n",
      "960/960 [==============================] - 20s 21ms/step - loss: 2.1045 - accuracy: 0.4847 - auc: 0.7248: 18s - loss: 2.1895 - acc - ETA: 1 - ETA: 8s - loss: 2.1447 - accuracy: 0.4723 - ETA: 7s - loss: 2.1469 - accuracy: 0.4727 - auc - ETA: 7s - loss: 2.146 - ETA: 6s - loss: 2.1381 - accuracy: 0.4756 - auc - ETA: 6s - loss: 2.1 - ETA: 3s - loss: 2.1224 - accuracy: 0. - ETA: 0s - loss: 2.1074 - accuracy: 0.4842 - auc:  - ETA: 0s - loss: 2.1079 - accuracy\n",
      "Epoch 6/15\n",
      "960/960 [==============================] - 20s 21ms/step - loss: 1.9737 - accuracy: 0.5255 - auc: 0.7527: 17s - loss: 1.9814 - accura - ETA: 15s - loss: 1.9930 - accuracy: 0. - ETA: 15s - loss: 1.9901 - accuracy: 0.5246 - auc: - ETA: 14s - loss: 1.9889 - ac - ETA: 12s - loss: 1.9915 - accuracy: 0.5230 - auc: 0 - ETA: 12s - loss: 1.9992 - accuracy: 0.5202 - auc - ETA: 12s - loss: 2.0020 - accuracy: 0.5205 - a - ETA: 11s - loss: 1.9973 - accuracy: 0.5220  - ETA: 10 - ETA: 0s - loss: 1.9746 - accuracy: 0.5252 - auc: 0.75\n",
      "Epoch 7/15\n",
      "960/960 [==============================] - 20s 21ms/step - loss: 1.8934 - accuracy: 0.5510 - auc: 0.7698:  - ETA: 15s - loss: 1.9034 - accuracy: 0.5465 - auc: 0.7 - ETA: 15s - loss: 1.9074 - - ETA: 13s - loss: 1.9102 - accuracy: 0.5503 - - ETA: 13s - loss: 1.9134 - accuracy: 0.5470 - auc: 0.766 - ETA: 13s - loss: 1.9165 - accuracy: 0.5 - ETA: 12s - loss: 1.9141 - accuracy: 0.5466  - ETA: 11s - loss: 1.9118 - accuracy: 0.5 - ETA: 10s - loss: 1.9041 - accuracy: 0.5516 - a - ETA: 9s - loss: 1.9 - ETA: 3s - ETA: 2s - loss: 1.8932 - accuracy: 0.5508 - auc: 0. - ETA: 1s - loss: 1.8 - ETA: 0s - loss: 1.8960 - accura\n",
      "Epoch 8/15\n",
      "960/960 [==============================] - 20s 21ms/step - loss: 1.7935 - accuracy: 0.5832 - auc: 0.7904: 20s - loss: 1.8745 - accu - ETA: 18s - loss: 1.8521 - accura - ETA: - ETA: 8s - loss: 1.8 - ETA: 7s - loss: 1.8146 - accuracy: 0. - ETA: 6s - loss: 1.8104 - accuracy: 0.5756 - auc: 0.78 - ETA: 6s - loss: 1.8100 - accuracy - ETA: 5s - ETA: 4s - loss: 1.7958 - accura - ETA: 3s - loss: 1.7968 - accuracy: 0.5819 - auc: 0.78 - ETA: 3s - loss: 1.7979 - accuracy: 0.58 - ETA: 3s - loss: 1.7997 - accuracy: 0.5814 - auc:  - ETA: 2s - loss:\n",
      "Epoch 9/15\n",
      "960/960 [==============================] - 21s 21ms/step - loss: 1.7094 - accuracy: 0.6091 - auc: 0.8079: 19s - loss: 1.6446 - accuracy: 0.6383 - auc: 0 - ETA: 19s - loss: 1.7017  - ETA: 17s - loss: 1.7210 - accuracy: 0.6049 - auc: 0 - ETA: 17s - loss: 1.7123 - accuracy: 0.6098 - - ETA: 16s - loss: 1.7096 - accuracy: 0.6119 - auc:  - ETA: 16s - loss: 1.7170 - ETA: 14s - loss: 1.7170 - accuracy: 0.6068 - auc:  - ETA: 13s - loss: 1.7122 - accuracy: 0.6068 - auc: 0.80 - ETA: 13s - loss: 1.7103 - accurac - ETA: 12s - loss: 1.7076 - accuracy: 0.606 - ETA: 11s - loss: 1.7083 - accuracy: 0.6073 - ETA: 10s - loss: - ETA:  - ETA: 5s - los - ETA: 0s - loss: 1.7115 - ac\n",
      "Epoch 10/15\n",
      "960/960 [==============================] - 20s 21ms/step - loss: 1.6345 - accuracy: 0.6382 - auc: 0.8240: 18s - loss: 1.6820 - accuracy: 0.623 - ETA: 17s - loss: 1.6988  - ETA: 15s - loss: 1.6944 - accuracy: 0.6207 - auc: 0.811 - ETA: 15s - loss: 1. - ETA: 12s - loss: 1.6744 - accuracy: 0.6245 - ETA: 12s - loss: 1.6735 - accuracy: 0.6252 - au - ETA: 11s - loss: 1.6682 - accurac - ETA: 10s - - ETA: 8s - loss: 1.6531 - accuracy: 0.6317 - auc - ETA: 8s - loss: 1.654 - ETA: 7s - loss: 1.6467 - accuracy:  - ETA:  - ETA: 5s - loss: 1.6408 - ac - ETA: 4s - loss: 1.6393 - accu - ETA: 3s - loss: 1.6390 - accuracy: 0.6379 - auc:  - ETA: 3s - loss: 1.6389 - ac - ETA: 2s - loss: 1.6375 - accuracy: 0.6381 - auc - ETA: 2s - loss: 1.6380 -  - ETA: 1s - loss: 1.6376 - accura - ETA: 0s - loss: 1.6363 - accuracy: 0.\n",
      "Epoch 11/15\n",
      "960/960 [==============================] - 20s 21ms/step - loss: 1.5814 - accuracy: 0.6554 - auc: 0.8357: 19s - loss: 1.6409 - accuracy: 0.6502 - auc: 0.82 - ETA: 19s - loss: 1.6225 - accuracy: 0.6567 - auc: 0 - ETA: 19s - loss: 1.6210 - accur - ETA: 17s - loss: 1.5973 - accuracy: 0.6500 - auc: 0.83 - ETA: 17s - loss: 1.598 - ETA: 15s - loss: 1.6052 - accuracy: 0.65 - ETA: 14s - loss: 1.6034 - accuracy: 0.652 - ETA: 13s - loss: 1.6081 - accura - ETA: 11s - - ETA: 7s - loss: 1.5964 - accuracy: 0.6499 - auc - ETA: 7s - loss: 1.5942 - accuracy: 0. - ETA: 6s - loss: 1.5923 - accuracy: 0.6503 - a - ETA: 6s - loss: 1.5924 - accuracy: 0.6502 - auc: 0. - ETA: 6s - loss: 1.5925 - accuracy: 0.6503 - a - E - ETA: 4s - loss: - ETA: 3s\n",
      "Epoch 12/15\n",
      "960/960 [==============================] - 20s 21ms/step - loss: 1.5235 - accuracy: 0.6820 - auc: 0.8490: 18s - loss: 1.5862 - accuracy: 0.6672  - ETA: 18s - loss: 1.5699 - accuracy: 0.6 - ETA: 16s - loss: 1.5770 - accura - ETA: 15s - loss: 1.5557 - accuracy: 0.6755 - - ETA: 14s - loss: 1.5478 - accuracy: 0.6780 - auc: 0.8 - ETA: 14s - loss: 1.5473 - accuracy: 0.6774 - auc: 0.84 - ETA: 14s - loss: 1.5471 - accuracy: 0.6765 - auc: 0. - ETA: 14s - loss: 1.5476 - accuracy: 0.6757 - auc: 0.84 - ETA: 14s - loss: 1.54 - ETA: 11s - loss: 1.5358 - accuracy: 0.6808 - auc: 0. - ETA: 11s - loss: 1.5385 - - ETA: 0s - loss: 1.5246 - accuracy: 0.6820 - ETA: 0s - loss: 1.5235 - accuracy: 0.6820\n",
      "Epoch 13/15\n",
      "960/960 [==============================] - 20s 21ms/step - loss: 1.4703 - accuracy: 0.6965 - auc: 0.8597: 15s - loss: 1.5024 - accuracy: 0.6915 -  - ETA: 14s - loss: 1.5013 - accuracy: 0.6910 - auc: 0. - ETA - ETA: 7s - loss: 1.4776 - accuracy: 0.6923 - - ETA: 3s - loss: 1.4750 - accuracy:  - ETA: 1s - loss: 1\n",
      "Epoch 14/15\n",
      "960/960 [==============================] - 20s 21ms/step - loss: 1.4060 - accuracy: 0.7234 - auc: 0.8716: 19s - loss: 1.4363 - accuracy: 0.7194 - auc: 0.867 - ETA: 19s - loss: 1.435 - ETA: 2s - loss: 1.4141 - accuracy: 0.7219 - auc: 0. - ETA: 2s - loss: 1.4138 - accuracy: 0.7222 - auc: 0. - ETA: 2s - loss: 1.4133 \n",
      "Epoch 15/15\n",
      "960/960 [==============================] - 20s 21ms/step - loss: 1.3565 - accuracy: 0.7394 - auc: 0.8814: 19s - loss: 1.3742 - accuracy: 0.7474 - auc: 0. - ETA: 18s - loss: 1.3556 - accuracy: 0.7490 - auc: 0. - ETA: 18s - loss: 1.354 - ETA: 16s - loss: 1.3492 - accuracy: 0.7417 - auc: 0. - ETA: 16s - loss: 1.3524 - accuracy: 0.7394 - auc: 0.882 - ETA: 16s - loss: 1.3496 - - ETA: 14s - loss: 1.3617 - accuracy: - ETA:  - ETA: 8s - loss: 1.3703 -  - ETA: 5s - loss: 1.3669 - accuracy: 0.7350 - auc - ETA: 5s - loss: 1.3652 - accuracy: 0.7353 - - ETA: 5s - loss: 1.3667 - accuracy: 0.7354 - a - ETA: 4s - loss: 1.3660 - accuracy: 0.7359 - auc - ETA: 4s - loss: 1.3671 - accuracy:  - ETA: 2s - loss: 1.3656 - accuracy: 0.7360 - a - ETA: 2s - loss: 1.3632  - ETA: 1s - loss: 1\n",
      "########################################################################################################################\n",
      "Fold ended\n",
      "########################################################################################################################\n",
      "Test Score\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 2.4708 - accuracy: 0.4156 - auc: 0.6267\n",
      "########################################################################################################################\n",
      "Epoch 1/15\n",
      "961/961 [==============================] - 21s 21ms/step - loss: 1.4995 - accuracy: 0.7089 - auc: 0.8577: 18s - loss: 1.5568 - accuracy: 0.6953 - auc:  - ETA: 18s - loss: 1.5547 - accuracy: 0.6943 - auc:  - ETA: 18s - loss: 1.5525 - accuracy: 0.6990  - ETA: 17s - loss: 1.5576 - accuracy: 0.6963 - auc: 0.84 - ETA: 17s - loss: 1.55 - ETA: 15s - loss: 1.5217 - accuracy: 0.7078 - auc - ETA: 14s - loss: 1.5175 - accuracy: 0.7077 - auc: 0.85 - ETA: 14s - loss: 1.5208 - accuracy: 0.7079 - auc - ETA: 14s - loss: 1.5208 - accuracy: 0.7054 - auc - ETA: 13s - loss: 1.5219 - accuracy: 0.7046 - au - ETA: 13s - loss: 1.5224 - accuracy: 0.7039 - auc: - ETA: 12s - loss: 1.5221 - accuracy: 0.7045 - auc: 0.8 - ETA: 12s - loss: 1.5191 - ac - ETA: 10s - loss: 1.5131 - accuracy: 0.7066 - auc - ET - ETA: 8s - loss: 1.5197 - accuracy: 0.7060 - - ETA: 8s - loss: 1.5 - ETA: 7s - loss: 1.5241 - accuracy:  - ETA: 6s - los - ETA: 5s - loss: 1.5132  - ETA: 0s - loss: 1.4995 - accu\n",
      "Epoch 2/15\n",
      "961/961 [==============================] - 20s 21ms/step - loss: 1.4238 - accuracy: 0.7311 - auc: 0.8728: 19s - loss: 1.4688 - accuracy: 0.7045 - auc - ETA: 18s - loss: 1.4504 - accuracy: 0.7108 - - ETA: 18s - loss: - ETA: 15s - loss: 1.4446 - accuracy: 0.7 - ETA: 14s - loss: 1.4531 - accuracy: 0.7142 - a - ETA: 14s - loss: 1.4507 - accuracy: 0.7150 -  - ETA: 13s - loss: 1.4472 - accuracy: 0.7185 - auc: 0.868 - ETA: 13s - loss: 1.4467 - accuracy: 0.7188 - auc:  - ETA: 12s - loss: 1.4483 - accurac - ETA: 11s - loss: 1.4562 - accuracy: 0.7162 -  - ETA: 10s - loss: - ETA: 9s - loss: 1.4479 - accuracy:  - ETA: 8s - loss: - ETA: 7s - loss: 1.4478 - accuracy: 0.7214 - auc:  - ETA: 7s - loss: 1.4471 - accuracy: 0.72 - ETA: 6s - loss: - ETA: 5s - loss: 1.4448 - accuracy: 0.7231 - auc: 0. - ETA: 5s - loss: 1.4444 - accuracy: 0.7234 - auc: 0.86 - ETA: 5s - loss: 1.4443 - accuracy: 0.7232 - auc: 0. - ETA: 5s - loss: 1.4440 - accuracy: 0.7233 - - ETA: 4s - loss: 1.4392 - accuracy: 0.7247 - auc: 0. - ETA: 4s - loss: 1.4379 - accuracy - ETA: 4s - - ETA: 1s - loss: 1.4\n",
      "Epoch 3/15\n",
      "961/961 [==============================] - 20s 21ms/step - loss: 1.3588 - accuracy: 0.7500 - auc: 0.8836: 20s - loss: 1.3621 - accuracy: 0.7 - ETA: 19s - loss: 1.4165 - acc - ETA: 17s - loss: 1.377 - ETA: 15s - loss: 1.3666 - accuracy: 0. - ETA: 13s - loss: 1.3746 - accuracy: 0.7496 - auc: - ETA: 13s - loss: 1.3704 - accuracy: 0.751 - ETA: 12s - loss: 1.3777 - accuracy: 0.7464 - auc: 0.8 - ETA: 12s - loss: 1.3755 - accuracy: 0.7 - ETA: 11s - loss: 1.3833 - accuracy: - ETA: 10s - loss: 1.3799 - accuracy: 0.7454 - auc: 0.87 - ETA: 10s - loss: 1.3789 - accuracy: 0.7459 - auc: 0.8 - ETA: 9s - loss: 1.3799 - accu - ETA: 9s - los - ETA: 7s - loss: 1.3733 - accuracy: 0.7464 - ETA: 7s - loss: 1.3734 - accu - ETA: 6s - loss: 1.3 - ETA: 3s - loss: 1.3647 - accuracy: 0.7485 - a - ETA: 1s - loss: 1.3618 - accuracy - ETA: 1s - loss: 1.3604 - accuracy: 0.7495 - auc - ETA: 0s - loss: 1.3597 - accu\n",
      "Epoch 4/15\n",
      "961/961 [==============================] - 20s 21ms/step - loss: 1.3144 - accuracy: 0.7628 - auc: 0.8925: 17s -  - ETA: 11s - loss: 1.3207 - accuracy: 0.760 - ETA: 10s - loss: 1.3179 - accuracy: 0.7620 - ETA: 9s - loss: 1.3176 - accuracy: 0.7619 - ETA - ETA: 7s - loss: 1.3 - ETA: 6s - loss: 1.3142 - accuracy: 0.7626 - ETA: 6s - loss: 1.3165 - accuracy: 0.7617 - a - ETA: 5s - loss: 1.3156 - ac - ETA: 5s - loss: 1.3138 - accu - ETA: 4s - loss: 1.3132 - accuracy: 0.76 - ETA: 3s - loss: 1.3154 - accura - ETA: 3s - loss: 1.3154 - accuracy: 0.7631 - ETA: 2s - loss: 1.3169 - accuracy: 0.76 - ETA: 2s - loss: 1.3177 - accu - ETA: 1s - loss: 1.3149 - accuracy: 0. - ETA: 0s - loss: 1.3152 - \n",
      "Epoch 5/15\n",
      "961/961 [==============================] - 20s 21ms/step - loss: 1.2720 - accuracy: 0.7754 - auc: 0.8995: 19s - loss: 1.2659 - accuracy: 0.7853 -  - ETA: 19s - loss: 1.2877 - accuracy: 0.7739 -  - ETA: 1 - ETA: 15s - loss: 1.2994 - accuracy: 0.7672 - auc: 0.893 - - ETA: 11s - loss: 1.2867 - accuracy: 0.7723 - auc:  - ETA: 11s - loss: 1.2867 - accuracy: 0.7726 - a - ETA: 11 - ETA: 9s - loss: 1.2859 - ac - ETA: 6s - loss: 1.2815 - accu - ETA: 5s - loss: 1.2797 - accuracy: 0.7745 - auc: 0. - ETA: 5s - loss: 1.2 - ETA: 4s - loss: 1.2780 - accuracy: 0.7743 - ETA: 3s - loss: 1.2773 - accuracy: 0.7744 - auc: 0.89 - ETA: 3s - loss: 1.2765 - accuracy: 0.7747 - auc - ETA: 3s - loss: 1.2764  - ETA: 2s - loss: 1.2740 -  - ETA: 1s - loss: 1.2731 - accuracy: 0.77 - ETA: 1s - loss: 1.2728 - ac - ETA: 0s - loss: 1.2725 - accuracy: 0.7753 - auc: 0. - ETA: 0s - loss: 1.2720 - accuracy: 0.7754 - auc:  - ETA: 0s - loss: 1.2709 - accuracy: 0.7758 - auc: \n",
      "Epoch 6/15\n",
      "961/961 [==============================] - 20s 21ms/step - loss: 1.2427 - accuracy: 0.7871 - auc: 0.9067: 18s - los - ETA: 16s - loss: 1.2838 - acc - ETA: 14s - loss: 1.2732 - accuracy: 0.7778 - - ETA: 13s - loss: 1.2687 - accuracy - ETA: 12s - loss: 1.2693 - accuracy: 0.7795 - a - ETA: 11s - loss: 1.2689 - accuracy: 0.7797 - auc: 0.9 - ETA: 11s - loss: 1.2709 - accuracy: 0.7789 -  - ETA: 10s - loss: 1.2640 - accuracy: 0.7801 - auc: 0.90 - ETA: 10s - loss: 1.2639 - accuracy: 0.7802 - auc: 0 - ETA: 10s - loss: 1.2613 - accuracy: 0.7822 - auc: 0.902 - ETA: 10s - loss: 1.2609 -  - ETA: 7s - loss: 1.2528 - accuracy: 0.7833 - auc: 0.90 - ETA: 7s - loss: 1.2527 - accuracy: 0.7831 - auc: 0.90 - ETA: 7s - loss: 1.2533 - accuracy: 0.7831 - auc: 0.90 - ETA: 7s - loss: 1.2538 - accuracy: 0.7830 - a - ETA: 6s - loss: 1.2541 - accuracy: 0.7823 - auc:  - ETA: 6s - - ETA: 5s - loss: 1.2521 - accuracy: 0.7835 - auc: 0.90 - ETA: 5s - loss: 1.2521 - accuracy: 0.7836 - auc: 0.90 - ETA: 5s - loss: 1.2523 - accuracy: 0.7838 - auc: 0.90 - ETA: 5s - loss: 1.2521 - accuracy: 0.7836 - auc:  - ETA: 5s - loss: 1.2512 - accu - ETA: 4s - loss: 1.2502 - accuracy: 0.7846 - auc: 0. - ETA: 4s - loss: 1.2489 - accuracy: 0.7849 - auc: 0. - ETA: 3s - loss: - ETA: 2s - loss: 1.2\n",
      "Epoch 7/15\n",
      "961/961 [==============================] - 20s 21ms/step - loss: 1.1954 - accuracy: 0.8073 - auc: 0.9142: 19s - loss: 1 - ETA: 17s - loss: 1.1875 -  - ETA: 15s - loss: 1.1915 - accuracy: 0.8035 - ETA: 14s - loss: 1.1899 - accuracy: 0.8063 - auc: 0.91 - ETA: 14s - loss: 1.1912 - accuracy: 0.8060 - auc: 0.914 - ETA: 1 - ETA: 11s - loss: 1.1908 - accuracy: 0.8103 - auc: 0.91 - ETA: 11s - loss: 1.1906 - accuracy: 0.8108 - auc: 0.915 - ETA: 11s - loss: 1.1911 - accuracy: 0.8105 - auc: 0.915 - ETA: 11s - loss: 1.1919 - accuracy: 0.8098 - auc: 0.915 - ETA: 11s - loss: 1.1924 - accuracy: 0.8094  - ETA: 6s - loss: 1.1924 - accuracy: 0.8091 - ETA: 6s - loss: 1.1929 - accuracy: 0.8087 - auc: 0.91 - ETA: 6s - loss: 1.1 - ETA: 1s - loss: 1.1952 - accuracy: 0.8078 - ETA: 1s - loss: 1.1957 - accuracy: 0.8076 - auc:  - ETA: 0s - loss: 1.1955 - accuracy: 0.80 - ETA: 0s - loss: 1.1949 - accuracy: 0.8075\n",
      "Epoch 8/15\n",
      "961/961 [==============================] - 20s 21ms/step - loss: 1.1513 - accuracy: 0.8244 - auc: 0.9227: 18s - loss: 1.1388 - accuracy: 0.8396 - auc: 0.9 - ETA: 18s - loss: 1.1445 - accuracy: 0.8387 - auc - ETA: 18s - loss: 1.1617 - accuracy: 0.8296 - au - ETA: 17s - loss: 1.1471 - accuracy: 0.8326 - ETA: 17s - loss: 1.1678 - accuracy: 0.8263  - ETA: 16s - loss: 1.1719 - accuracy: 0.8252 - auc:  - ETA: 16s - loss: 1 - ETA: 13s - loss: 1.1631 - accuracy: 0.8275 - - ETA: 12s - loss: 1.1629 - accuracy: 0.8263 - auc:  - ETA: 12s - loss: 1.1 - ETA: 10s - loss: 1.1632 - accuracy: 0.8240 - auc: 0.92 - ETA: 10s - loss: 1.1645 - acc - ETA: 7s - loss: 1.1588 - accuracy: 0.8243 - auc: 0.92 - ETA: 7s - loss: 1.1584 - accuracy: 0.8245 - - ETA: 7s - loss: 1.1583 - ac - ETA: 6s - loss: 1.1599 - accuracy: 0.8224 - - ETA: 6s - l - ETA: 2s - loss: 1.1507 - accuracy: 0.8245 - auc: 0.92 - ETA: 2s - loss: 1.1511 - accuracy: 0.8242 - auc: 0.92 - ETA: 2s - loss: 1.1507 - accuracy: 0.8243 - a - ETA: 0s - loss: 1.1520 - ac\n",
      "Epoch 9/15\n",
      "961/961 [==============================] - 20s 21ms/step - loss: 1.1330 - accuracy: 0.8330 - auc: 0.9260: 17s - loss: 1.1293 - accuracy: 0.8252 - auc: 0 - ETA: 17s - loss: 1.1374 -  - ETA: 15s - loss: 1.1302 - accuracy: 0.8284 - auc: 0 - ETA: 15s - loss: 1.1293 - accuracy: 0.8285 - ETA: 14s - loss: 1.1310 - accuracy: - ETA: 8s - loss: 1.134 - ETA: 7s - loss: 1.1336 - accuracy: 0.8317 - - ETA: 6s - - ETA: 5s - loss: 1.1354 - accuracy: 0. - ETA: 4s - loss: 1.1353 - accu - ETA: 4s - ETA: 2s - loss: 1.1359 - accuracy: 0.8317 - - - ETA: 0s - loss: 1.1323 - accuracy: 0.8332 - ETA: 0s - loss: 1.1317 - accuracy: 0.8336\n",
      "Epoch 10/15\n",
      "961/961 [==============================] - 20s 21ms/step - loss: 1.1157 - accuracy: 0.8339 - auc: 0.9289: 19s - loss: - ETA: 17s - loss: 1.1425 - accuracy: 0.8275 -  - ETA: 16s - loss: 1.1443 - accuracy: 0.8 - ETA: 15s - loss: 1.1418 - accuracy: 0.8210 - au - ETA: 14s - loss: 1.1402 - accuracy: 0.8248 - auc: 0.925 - ETA: 14s - loss: 1.1394 - accuracy: 0.8251 - auc: - ETA: 14s - loss: 1.1391 - accur - ETA: 4s - loss: 1.1172 - accuracy: 0.8354 - auc:  - ETA: 4s - loss: 1.1176 -  - ETA: 3s - loss: 1.1175 - accuracy: 0.8344 - auc:  - ETA: 3s - loss: 1.1179 - accuracy: 0.8341 - auc: 0.92 - ETA: 3s - ETA: 1s - loss: 1.1168 - accu - ETA: 0s - loss: 1.1152 - accuracy: 0.8341 - - ETA: 0s - loss: 1.1156 - accuracy\n",
      "Epoch 11/15\n",
      "961/961 [==============================] - 20s 21ms/step - loss: 1.0879 - accuracy: 0.8477 - auc: 0.9345: 19s - loss: 1.1346 - accuracy: 0.8342 - - ETA: 18s - loss: 1.1126 - accuracy: 0.8378 - auc - ETA: 18s - loss: 1.1169 - accuracy: 0.8391 - auc:  - ETA: 17s - loss: 1.1059 - accuracy: 0.8419 - auc: 0 - ETA: 17s - loss: 1.1041 - ETA: 15s - loss: 1.0816 - accuracy: - ETA: 14s - loss: 1.0853 - a - ETA: 12s - loss: 1.0937 - accurac - ETA: 11s - loss: 1.1057 - accuracy: 0 - ETA: 10s - loss: 1.0986 - accuracy: 0.8429 - auc: 0.932 - ETA: 10s - loss: 1.0990 - accurac - ETA: 9s - loss: 1.0951 - accuracy: 0.8443 - auc: 0.93 - ETA: 9s - loss: 1 - ETA:  - ETA: 6s - loss: 1.0890  - ETA: 5s - loss: 1.0875 - accuracy: 0.8473 - - ETA: 5s - loss: 1.0861 - accuracy:  - ETA: 4s - loss: 1.0 - ETA: 1s - loss: 1.0885 - accuracy: 0.8465 - auc: 0. - ETA: 1s - loss: 1.0895 - accuracy:  - ETA: 1s - loss: 1.0895 - accuracy: 0.8465 - a - ETA: 0s - loss: 1.0889 - accu - ETA: 0s - loss: 1.0878 - accuracy: 0.8478 - auc: 0.93\n",
      "Epoch 12/15\n",
      "961/961 [==============================] - 20s 21ms/step - loss: 1.0566 - accuracy: 0.8607 - auc: 0.9394: 19s - loss: 1.0312 - accuracy: 0.8789 - auc:  - ETA: 18s - loss: - ETA: 16s - loss: 1.06 - ETA: 14s - loss: 1.0663 - accuracy: 0.8600 - auc: 0. - ETA: 14s - loss: 1.0658 - accuracy: 0.8607 - auc: 0. - ETA: 13s - loss: 1.0651 - accuracy: 0.8610 - auc:  - ETA: 13s - loss: 1.0654 - accuracy: 0.8606 -  - ETA: 12s - loss: 1.0698 - accuracy: 0.8577 - auc: 0.938 - ETA: 12s - loss: 1.0694 - accuracy: 0.8575 - auc: 0.938 - ETA: 12s - loss: 1.0685 - accuracy: 0.8579 -  - ETA: 11s - loss: 1.0637 - accuracy: 0.85 - ETA: 11s - loss: 1.0634 - accuracy: 0.8579 - auc: 0.9 - ETA: 10s - loss: 1.0637 - accuracy: 0.8579 - auc: 0. - ETA: 10s - loss: 1.0643 - accuracy: 0.8578 - auc:  - ETA: 10s - loss: 1.0635 - accuracy: 0.8582  - ETA: 7s - loss: 1.0607 - accuracy: 0.8582 - auc:  - ETA: 7s - los - ETA: 6s - loss: 1.0600 - accuracy: 0.8598 - a - E - ETA: 4s - loss: 1.0580 - accuracy: 0.8602 - auc: 0.93 - ETA: 4s - loss: 1.0585 - accuracy: 0.8596 - auc - ETA: 4s - loss: 1.0583 - accuracy: 0.8598 - auc - ETA: 4s - loss: 1 - ETA: 1s - loss: 1.0573 - accuracy: 0.8602 - a - ETA: 0s - loss: 1.0567 - accu - ETA: 0s - loss: 1.0571 - accuracy: 0.8605 - auc: \n",
      "Epoch 13/15\n",
      "961/961 [==============================] - 20s 21ms/step - loss: 1.0372 - accuracy: 0.8658 - auc: 0.9444: 18s - lo - ETA: 16s - loss: 1.0594 - accuracy: 0.8572 - auc: 0.93 - ETA: 16s - loss: 1.0601 - accuracy: 0.8587 - a - ETA: 15s - loss: 1.0581 - accuracy: 0.8623 - auc: 0.940 - ETA: 15s - loss: 1.0561 - accuracy: 0.8633 - a - ETA: 11s - loss: 1.0517 - accuracy: 0.8606 - auc: 0. - ETA: 11s - loss: 1.0515 - accuracy: 0.8608 - auc:  - ETA: 10s - loss: 1.0545 - accuracy: 0.8593 - ETA: 10s - loss: 1.0491 - accuracy: 0.8604  - ETA: 9s - loss: 1.0489 - accuracy: 0. - ETA: 9s - loss: 1.0470 - accuracy: 0.8620 - auc - ETA: 8s - loss: 1.0462 - accuracy: 0.8618 - auc - ETA: 8s - loss: 1.0448 - ac - ETA: 7s - loss: 1.0445 - accuracy: 0.8618 - auc: 0.94 - ETA: 7s - loss: 1.0442 - accuracy: 0.8618 - auc: 0.94 - ETA: 7s - loss: - ETA: 6s - loss: 1.0408 - accuracy: 0.86 - ETA: 5s - loss: 1.0419 - accuracy: 0. - ETA: 5s - loss: 1.0408 - accuracy: 0.8644 - auc: 0.94 - ETA: 5s - loss: 1.0412 - accura - ETA: 4s - loss: 1.038 - ETA: 3s - loss: 1.039 - ETA: 2s - loss: 1.0386 - accuracy: 0.8652 - auc: 0. - ETA: 2s - loss: 1.0379 - accuracy: 0.8653 - auc: 0. - ETA: 2s - loss: 1.0388 - accuracy:  - ETA: 1s - loss: 1.0393 - accuracy:  - ETA: 1s - loss: 1.0386 - accuracy: 0.8653 - auc:  - ETA: 0s - loss: 1.0392 - accuracy: 0.8652 - auc: 0. - ETA: 0s - loss: 1.0390 - accuracy: 0. - ETA: 0s - loss: 1.0373 - accuracy: 0.8658 - auc:  - ETA: 0s - loss: 1.0372 - accuracy: 0.8658 - auc: 0.94\n",
      "Epoch 14/15\n",
      "961/961 [==============================] - 20s 21ms/step - loss: 1.0208 - accuracy: 0.8754 - auc: 0.9454: 21s - loss: 1.0613 - accuracy: 0.8398 - a - ETA: 19s - loss: 1. - ETA: 17s - loss: 1.0245 - accuracy: 0.8725 - auc: 0.9 - ETA: 17s - loss: 1.0314 - accuracy: 0.8711 -  - ETA: 16s - loss: 1.0257 - accuracy: 0.8737 - a - ETA: 15s - loss: 1.0202 - accuracy: 0 - ETA: 14s - loss: 1.019 - ETA: 0s - loss: 1.0216 - accuracy: 0.8746 - auc - ETA: 0s - loss: 1.0220 - accuracy: 0.87\n",
      "Epoch 15/15\n",
      "961/961 [==============================] - 20s 21ms/step - loss: 1.0036 - accuracy: 0.8814 - auc: 0.9485: 13s - loss: 1.0160 - - ETA: 11s - loss: 1.0125 - accu - ETA: 9s - loss: 1.0136 - accuracy: 0 - ETA: 9s - loss: 1.0125 - accu - ETA: 8s - loss: 1.0123 - accuracy: 0.8798 - a - ETA: 8s - loss: 1.0107 - accuracy: 0.8806 - auc: 0. - ETA: 8s - loss: 1.0104 - ac - ETA: 7s - loss: 1 - ETA: 2s - los - ETA: 1s - loss: 1.0039 - accu - ETA: 0s - loss: 1.0045 - accu\n",
      "########################################################################################################################\n",
      "Fold ended\n",
      "########################################################################################################################\n",
      "Test Score\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 1.3434 - accuracy: 0.7410 - auc: 0.8803\n",
      "########################################################################################################################\n",
      "Epoch 1/15\n",
      "960/960 [==============================] - 21s 22ms/step - loss: 1.0572 - accuracy: 0.8626 - auc: 0.9395: 19s - loss: 1.0837 - accuracy: - ETA: 18s -  - ETA: 16s - loss: 1.0634 - accuracy: 0.8652 - auc: 0.938 - ETA: 15s - loss: 1.0637 - accuracy: 0.8647 - auc: 0.938 - ETA: 15s - loss: 1.0641 - accuracy: 0.8646 - auc: 0.93 - ETA: 15s - loss: 1.0608 - accuracy: 0 - ETA: 14s - loss: 1.0579 - accuracy: 0.8659 - auc: 0.939 - ETA: 14s - loss: 1.0567 - accuracy: 0.8663 - auc: 0.940 - ETA: 14s - loss: 1.0553 - accuracy: 0.8666 - auc: - ETA: 14s - loss: 1.0585 - accuracy: 0 - ETA: 12s - loss: 1.0620 - accuracy: - ETA: 11s - loss: 1.0644 - accuracy: 0.8619 - auc - ETA: 11s - loss: 1.0612 - accuracy: 0.8631 - auc: 0. - ETA: 10s - loss: 1.0618 - accuracy: 0.8623 -  - ETA: 10s - loss: 1.0635 - accuracy: 0.8611 - au - ETA: 9s - loss: 1.0638 - accuracy: 0.86 - ETA: 9s - loss: 1.0600 - accuracy: 0.8631 - auc: 0.94 - E - ETA: 7s - loss: 1.0657 - accuracy: 0.8616 - a - ETA: 7s - loss: 1.0650 - accuracy: 0.8619 - auc - ETA - ETA: 2s - loss: 1 - ETA: 0s - loss: 1.058\n",
      "Epoch 2/15\n",
      "960/960 [==============================] - 21s 21ms/step - loss: 1.0336 - accuracy: 0.8704 - auc: 0.9430: 20s - loss: 1.0561 - accuracy: 0.8710 - au - ETA: 19s - loss: 1.0511 - - ETA: 17s - loss: 1.0364 - accuracy: 0.8729 - auc:  - ETA: 17s - loss: 1.0393 - accuracy: 0.8739 - auc: 0.941 - ETA: 16s - loss: 1.0400 - accuracy: 0.8732 - auc: 0.94 - ETA: 16s - loss: 1.0444 - accuracy: 0.871 - ETA: 15s - loss: 1.0406 - accuracy - ETA: 10s - loss: 1.0379 - accuracy: 0.8718 - auc:  - ETA: 10s - loss: 1.0377 - accuracy: 0.8725 - auc: 0. - ETA: 10s - loss: 1.0363 - accuracy: 0.8731 - a - ETA: 9s - loss: 1.0339 - accuracy: 0.8736 - auc: 0.94 - ETA: 9s - loss: 1.0345 - accuracy: 0.8734 - a - ETA: 9s - loss: 1.0332 -  - ETA: 8s - loss: 1.0327 - accuracy: 0.8741 - auc: 0.94 - ETA: 8s - loss: 1.0325 - accuracy - ETA: 7s - loss: 1.0352 - accuracy: 0.8732 - auc: 0. - ETA: 7s - loss: 1.0364 - accuracy:  - ETA: 6s - loss: 1.0374 - accuracy: 0.8715 - - ETA: 6s - loss: 1.0368 - accuracy - ETA: 5s - loss: 1.0388 - accuracy: 0.8706 - - ETA: 5s - loss: 1.0396  - ETA:  - ETA: 1s - loss: - ETA: 0s - loss: 1.0337 - accuracy: 0.8707 - auc: \n",
      "Epoch 3/15\n",
      "960/960 [==============================] - 20s 21ms/step - loss: 1.0118 - accuracy: 0.8775 - auc: 0.9462: 18s - loss: 1.0189 - accur - ETA: 16s - loss: 1.015 - ETA: 3s - loss: 1.0151 - accuracy: 0.8771 - auc: 0.94 - ETA: 3s - loss: 1.0149 - accuracy: 0.87 - ETA: 2s - loss: 1.0136 - accuracy: 0.8772 - a - ETA: 2s - loss: 1.0131 - accuracy: 0.8772 - auc: 0. - ETA: 2s - loss: 1.0126 - accuracy: 0.8774 - ETA: 2s - loss: 1.0116 - accuracy: 0. - ETA: 1s - loss: 1.0124 - accuracy:  - ETA: 0s - loss: 1.0122 - accuracy: 0.8775 - auc:  - ETA: 0s - loss: 1.0127 - accu\n",
      "Epoch 4/15\n",
      "960/960 [==============================] - 20s 21ms/step - loss: 0.9858 - accuracy: 0.8922 - auc: 0.9505: 19s - loss: 0.9821 - accuracy: 0.9007 - auc: 0.  - ETA: 12s - loss: 0.9843 - accuracy: 0.8950 - auc: 0. - ETA: 11s - loss: 0.9874 - accu - ETA: 8s - ETA: 1s - loss: 0.9846 - accuracy: 0.89 - ETA: 1s - loss: 0.9846 - accuracy: 0.8930 - auc: 0.95 - ETA: 1s - loss: 0.9849 - accura - ETA: 0s - loss: 0.9854 - accuracy: 0.8923 - auc - ETA: 0s - loss: 0.9857 - accuracy: 0.8922 - auc: \n",
      "Epoch 5/15\n",
      "960/960 [==============================] - 21s 21ms/step - loss: 0.9747 - accuracy: 0.8949 - auc: 0.9523: 18s - loss: 0.9700  - ETA: 18s - loss: 0.9854 - accuracy: 0.8879 - auc: 0. - ETA: 18s - loss: 0.9760 - accuracy: 0.8 - ETA: 17s - loss: 0.9898 - accuracy: 0.8894 - auc: 0.95 - ETA: 16s - loss: 0.9928 - accuracy: 0.8885 - a - ETA: 16s - loss: 0.9950 - accuracy: - ETA: 15s - loss: 0.9953 - accuracy: 0.8881 - ETA: 10s - loss: 0.9906 - accuracy: 0.8890 - auc: - ETA: 10s - loss: 0.9897 - accuracy: 0.8903 -  - ETA: 9s - loss: 0.9862 - accuracy: 0.8911 - auc - ETA: 9s - loss: 0.9868 - accura - ETA: 8s - loss: 0.9875 - accuracy: 0.8910 - auc: 0.95 - ETA: 8s - loss: 0.9874 - accuracy: 0.89 - ETA: 8s - loss: 0.9859 - accuracy: 0.8915 - auc: 0.95 - ETA: 8s - ETA: 6s - loss: 0.9843  - ETA: 5s - loss: 0.9842 - accuracy: 0.8916 - auc: 0.95 - ETA: 5s - loss: 0.9845 - accuracy: 0.8917 - a - ETA: 5s - los - ETA: 4s - loss: 0.9807 - accuracy: 0.8922 - auc - ETA: 4s - loss: 0.9807 - accuracy:  - ETA: 1s - loss: 0.9773 - accuracy: 0.8940 - auc: 0.95 - ETA: 1s - loss: 0.9771 - accuracy: 0.8941 - a - ETA: 1s - loss: 0.9774 - accura - ETA: 0s - loss: 0.9771 - accu\n",
      "Epoch 6/15\n",
      "960/960 [==============================] - 20s 21ms/step - loss: 0.9511 - accuracy: 0.9021 - auc: 0.9569: 19s - loss: 0.9929 - - ETA: 17s - loss: 0.9620 - accuracy:  - ETA: 16s - loss: 0.9571 - accuracy: 0.89 - ETA: 15s - loss: 0.9591 - accuracy: - ETA: 14s - loss: 0.9544 - accura - ETA: 12s - loss: 0.9580 - accuracy: 0.89 - ETA: 11s - loss: 0.9562 - accuracy: 0.8993 - auc: 0.95 - ETA: 11s - loss: 0.9543 - accuracy: 0.9003 - auc: 0. - ETA: 11s - loss: 0.9553 - accuracy: 0.9 - ETA: 10s - loss: 0.9578 - a - ETA: 5s - loss: 0.9523 - accuracy: 0.9026 - auc: 0. - ETA: 5s - loss: 0.9520 - accuracy: 0.9028 - a - ETA: 5s - l - ETA: 4s - loss: 0.9527 -  - ETA: 3s - loss: 0.954 - ETA: 2s - loss: 0.9529 - accuracy: 0.90 - ETA: 1s - loss: 0.9528 - accuracy: 0.9020 - auc - ETA: \n",
      "Epoch 7/15\n",
      "960/960 [==============================] - 21s 21ms/step - loss: 0.9395 - accuracy: 0.9041 - auc: 0.9579: 19s - loss: 0.9489 - accuracy: 0.9 - ETA: 17s - loss: 0.9366 - accuracy: 0. - ETA: 16s - los - ETA: 14s - loss: 0.9393 - accuracy - ETA: 13s - loss: 0.9417 - accuracy: 0.9061 - a - ETA: 12s - l - ETA: 1s - loss: 0.9409 - accuracy: 0. - ETA: 0s - loss: 0.9406 - accuracy\n",
      "Epoch 8/15\n",
      "960/960 [==============================] - 21s 21ms/step - loss: 0.9262 - accuracy: 0.9094 - auc: 0.9599: 19s - loss: 0.9099 - accuracy:  - ETA: 18s - loss: 0.9157 - accuracy: 0.906 - ETA: 17s - loss: 0.9187 - accuracy: 0.9100 - auc: 0. - ETA: 17s - loss: 0.9200 - accuracy:  - ETA: 16s - loss: 0.9337 - accuracy: 0.9070 - auc: 0.95 - ETA: 16s - loss: 0.9318 - accuracy:  - ETA: 14s - loss: 0.9249 - accuracy: 0.9114 - auc: - ETA: 14s - loss: 0.9284 - accuracy: 0.9109 - auc: 0. - ETA: 14s - loss: 0.9276 - accuracy: 0.9113 - auc - ETA: 13s - loss: 0.9298 - accuracy: 0.9 - ETA: 12s - loss: 0.9316 - accuracy: 0.9096 - auc: 0.9 - ETA: 0s - loss: 0.9254 - accura\n",
      "Epoch 9/15\n",
      "960/960 [==============================] - 20s 21ms/step - loss: 0.9092 - accuracy: 0.9179 - auc: 0.9629: 19s - loss: 0.9112 - accuracy: 0.9135 - auc - ETA: 19s - loss: 0.9209 - accura - ETA: 18s - loss: 0.9123 - accuracy: 0.9179 - auc: 0 - ETA: 18s - loss: 0.9139 - accurac - ETA: 16s - loss: 0.9163 - accuracy: 0.9138 - auc: 0.96 - ETA: 16s - loss: 0.9158 - accuracy: 0.9146 - auc - ETA: 16s - loss: 0.9166 - accuracy: 0.9155 - auc: 0.960 - ETA: 15s - loss: 0.9161 - accu - ETA: 14s - loss: 0.9109 - accuracy - ETA: 13s - loss: 0.9100 - accuracy: 0.9164 - auc:  - ETA: 12s - loss: 0.9103 - accuracy: 0.9164 - auc: 0.96 - ETA: 12s - loss: 0.9121 - accuracy: 0.9157 - auc: 0 - ETA: 12s - loss: 0.9134 - accuracy: 0.9143 - auc: 0.9 - ETA: 12s - loss: 0.9131 - accuracy: 0.9146 - auc: 0.96 - ETA: 11s - loss: 0.9149 - accuracy - ETA: 8s - loss: 0.9118 - accuracy: 0.9160 - auc: 0. - ETA: 8s - loss: 0.9114  - ETA: 7s - loss: 0.9142 - accuracy: 0.9151 - a - ETA: 7s - loss: 0.9139 - accuracy: 0.9155 - auc - ETA: 6s - loss: 0.9147 - accuracy: 0.9152 - a - ETA: 4s - loss: 0.9132 - accuracy: 0.9165 - ETA: 2s - loss: 0.9099 - accu - ETA: 1s - loss: 0.9107 - accuracy: 0.9172 - auc: 0.96 - ETA: 1s - loss: 0.9105 - accuracy: 0.9172 - a - ETA: 1s - loss: 0.9099 - accuracy: 0.9175 - auc: 0. - ETA: 1s - loss: 0.9100 -  - ETA: 0s - loss: 0.9093 - accuracy: \n",
      "Epoch 10/15\n",
      "960/960 [==============================] - 20s 21ms/step - loss: 0.8996 - accuracy: 0.9216 - auc: 0.9644A: 14s - loss: 0.9013 - accu - ETA: 13s - loss - ETA: 8s - loss: 0.9049 - accuracy - ETA: 7s - loss: - ETA: 6s - l - ETA: 5s - los - ETA: 4s - loss: - ETA: 2s - loss: 0.8978 - accuracy: 0. - ETA: 2s - loss: 0.8983 - accuracy: 0.9226 - ETA: 1s - loss: 0.8985 - accuracy: 0.9224 - - ETA: 1s - loss: 0.8992 - accura - ETA: 0s - loss: 0.8994 \n",
      "Epoch 11/15\n",
      "960/960 [==============================] - 21s 21ms/step - loss: 0.8890 - accuracy: 0.9247 - auc: 0.9658: 21s - loss: 0.9020 - accuracy: 0.92 - ETA: 19s - loss: 0.8933 - accuracy: 0.9250 - auc: 0.96 - ETA: 19s - loss: 0.8 - ETA: 17s - loss: 0.8982 - accuracy: 0.9244 - au - ETA: 16s - loss: 0.8905 - accuracy: 0.9252 - auc: 0.96 - ETA: 16s - loss: 0.8912 - accuracy: 0.9245 - auc: 0.9 - ETA: 16s - loss: 0.8935 - accuracy: 0.9229 - auc: 0 - ETA: 16s - loss: 0.8904 - accuracy: 0.9248 - auc: 0. - ETA: 15s - loss: 0.8936 - ac - ETA: 14s - loss: 0.8900 - accuracy: 0.9242 - auc: 0. - ETA: 13s - loss: 0.8897  - ETA: 11s - loss: 0.8946 - accuracy: 0.9217 - au - ETA: 11s - l - - ETA: 7s - loss: - ETA: 6s - loss: 0.8897 - accuracy: 0.9238 - - ETA: 6s - loss: 0.889\n",
      "Epoch 12/15\n",
      "960/960 [==============================] - 21s 21ms/step - loss: 0.8784 - accuracy: 0.9312 - auc: 0.9676: 18s - loss: 0.9007 - accuracy: 0.9246 - - ETA: 17 - ETA: 14s - loss: 0.8923 - accuracy: 0 - ETA: 13s - loss: 0.8901 - accuracy: 0.927 - ETA: 12s - loss: 0.8925 - accuracy: 0.9272 - auc: 0.96 - ETA: 12s - loss: 0.8928 - accuracy: 0.9266 - auc: - ETA: 11s - loss: 0.8937 - accuracy: 0.9256 - - ETA: 11s - loss: 0.8943 - accuracy: 0.9259  - ETA: 10s - loss: 0.8924 - accuracy: 0.92 - ETA: 9s - loss: 0.8901 - accuracy: 0.9273 - - ETA: 9s - loss: 0.8886 - accuracy: 0.9280 - - ETA: 8s - loss: 0.8883 - accuracy: 0.9286 - a - ETA: 8s - loss: 0.8874 - accuracy:  - ETA: 8s - loss: 0.8860 - accuracy: 0.9295 - auc - E - ETA: 6s - loss: 0.8805 - accuracy: 0.9313 - auc - ETA: 5s - loss: 0.8801 - accuracy - ETA: 5s - loss: 0.8808 - accuracy: 0.93 - ETA: 4s - loss: 0 - ETA: 1s - loss: 0.8791 - accuracy: 0.9312 - - ETA: 1s - loss: 0.8787 - accuracy: 0.9312 - auc:  - ETA: 1s - loss: 0 - ETA: 0s - loss: 0.8783 - accuracy: 0.9313 - auc:  - ETA: 0s - loss: 0.8782 - accuracy: 0.9313 - auc: 0.96\n",
      "Epoch 13/15\n",
      "960/960 [==============================] - 21s 21ms/step - loss: 0.8641 - accuracy: 0.9344 - auc: 0.9694: 15s - loss: 0.8645 - accuracy: 0.9352 - auc: 0.9 - ETA: 15s - loss: 0.8666 - accuracy: 0.9348 - auc: 0.969 - ETA: 15s - loss: 0.8660 - accuracy: \n",
      "Epoch 14/15\n",
      "960/960 [==============================] - 21s 21ms/step - loss: 0.8602 - accuracy: 0.9342 - auc: 0.9704: 19s - loss: 0.8237 - accu - ETA: 18s - loss: 0.8535 - accuracy: 0. - ETA: 17s - loss: 0.8565 - accuracy: 0.935 - ETA: 16s - loss: 0.8585 - accuracy: 0.9330 - auc - ETA: 15s - loss: 0.8586 - accura - ETA: 14s - loss: 0.8616 - acc - ETA: 12s - loss: 0.8613 - accuracy: 0.9328 - auc: 0.970 - ETA: 12s - loss: 0.8605 - accuracy: 0.9332 - auc: 0.970 - ETA: 12s - loss: 0.8600 - accuracy: 0 - ETA: 11s - loss: 0.8615 - accuracy: 0.9333 - auc: 0.970 - ETA: 11s - loss: 0.8625 - accuracy: 0.933 - ETA: 10s - loss: 0.8619 - accuracy: 0.9343 - auc: 0.970 - ETA: 10s - loss: 0.8614 - accuracy: 0.9344 - auc: 0.970 - ETA: 10s - loss: 0.8618 - accuracy: - ETA: 9s - loss: 0.8606 - accuracy: 0.9346 - auc: 0. - ETA: 9s - loss: 0.8599 - accuracy: 0.9347 - auc:  - ETA - ETA: 5s - loss: 0.8589 - accuracy: 0.9346 - - ETA: 5s - loss: 0.8588 - accura - ETA: 3s - loss: 0.8595 - accuracy: 0.9343 - a - ETA: 2s - loss: 0.8597 - ac - ETA: 1s - loss: 0.8595 - accu - ETA: 1s - loss: 0.859 - ETA: 0s - loss: 0.8601 - accuracy: 0.9343 - auc: \n",
      "Epoch 15/15\n",
      "960/960 [==============================] - 21s 21ms/step - loss: 0.8475 - accuracy: 0.9382 - auc: 0.9722: 19s - loss: 0.8228 - accuracy: 0.9421 - auc:  - ETA: 19s - loss: 0.8400 - acc - ETA: 17s - loss: 0.8447 - accuracy: 0.9411 -  - ETA: 17s - loss: 0.8443 - ETA: 15s - loss: 0.8472 - accuracy: 0.938 - ETA: 14s - loss: 0.8480 - accuracy: 0.9371 - auc:  - ETA: 14s - loss: 0.8484 - accuracy: 0.9373 - auc: - ETA: 13s - loss: 0.8459 - accuracy: 0.9385 - auc: 0.972 - ETA: 13s - loss: 0.8466 - accuracy: 0.9383 -  - ETA: 12s - loss: 0.8476 - accuracy: 0.9382 - a - ETA: 12s - loss: 0.8496 - acc - ETA: 10s - - ETA: 8s - loss: 0.848 - ETA: 4s - loss: 0.848 - ETA: 3s - ETA: 1s - loss: 0.8482 - accuracy: 0.9384 - auc: 0.97 - ETA: 1s - loss: 0.8480 - accuracy: 0.9385 - auc: 0.97 - ETA: 1s - loss: 0.8478 - accuracy: 0.93 - ETA: 1s - loss: 0.8479 - accuracy: 0.9383 - auc: 0.97 - ETA: 1s - loss: 0.8479 - accuracy:  - ETA: 0s - loss: 0.8480 - accuracy: \n",
      "########################################################################################################################\n",
      "Fold ended\n",
      "########################################################################################################################\n",
      "Test Score\n",
      "114/114 [==============================] - 1s 7ms/step - loss: 1.0797 - accuracy: 0.8414 - auc: 0.9533\n",
      "########################################################################################################################\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1e239121d3f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdadelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m               metrics=['accuracy','AUC'])\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label_enc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'#'\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m120\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1132\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m       \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1134\u001b[1;33m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[0;32m   1135\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m           \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1381\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1382\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1383\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1138\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1139\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1140\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m                **kwargs):\n\u001b[0;32m    229\u001b[0m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[0;32m    232\u001b[0m         sample_weights, sample_weight_modes)\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m   1029\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m   \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1032\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_to_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 869\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 869\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0m_is_scipy_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1428\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m   \"\"\"\n\u001b[1;32m-> 1430\u001b[1;33m   return convert_to_tensor_v2(\n\u001b[0m\u001b[0;32m   1431\u001b[0m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[0;32m   1432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1434\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m   \u001b[1;34m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1436\u001b[1;33m   return convert_to_tensor(\n\u001b[0m\u001b[0;32m   1437\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1438\u001b[0m       \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\profiler\\trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1565\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1566\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1568\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[1;31m# Unused.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m   \"\"\"\n\u001b[1;32m--> 271\u001b[1;33m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[0;32m    272\u001b[0m                         allow_broadcast=True)\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    281\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tf.constant\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m   \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m   \u001b[1;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m   \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    104\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "images_ = all_image\n",
    "labels_ = all_label\n",
    "hist = []\n",
    "test_score_l = []\n",
    "for j, (train_idx, test_idx) in enumerate(folds.split(images_, labels_)):\n",
    "    X_train = np.array(images_)[train_idx]\n",
    "    y_train = np.array(labels_)[train_idx]\n",
    "    X_test = np.array(images_)[test_idx]\n",
    "    y_test = np.array(labels_)[test_idx]\n",
    "    \n",
    "    train_image = import_maps(X_train,y_train)\n",
    "    test_image = import_maps(X_test,y_test)\n",
    "    \n",
    "    train_images_all = [i[0] for i in train_image]\n",
    "    train_images_array = np.array(train_images_all)\n",
    "    #train_images_array=np.expand_dims(train_images_array,axis=3)\n",
    "    train_image_label = [i[1] for i in train_image]\n",
    "    train_image_label = np.array(train_image_label)\n",
    "    \n",
    "    test_images_all = [i[0] for i in test_image]\n",
    "    test_images_array = np.array(test_images_all)\n",
    "    #test_images_array=np.expand_dims(test_images_array,axis=3)\n",
    "    test_image_label = [i[1] for i in test_image]\n",
    "    test_image_label = np.array(test_image_label)\n",
    "    \n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    train_label_enc = enc.fit_transform(train_image_label.reshape(-1, 1)).toarray()\n",
    "    #val_label_enc = enc.fit_transform(val_y.reshape(-1, 1)).toarray()\n",
    "    test_label_enc = enc.fit_transform(test_image_label.reshape(-1, 1)).toarray()\n",
    "    \n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy','AUC'])\n",
    "    history = model.fit(train_images_array, train_label_enc, batch_size=16, epochs=15, verbose=1,shuffle=True)\n",
    "    hist.append(history)\n",
    "    print('#'*120)\n",
    "    print('Fold ended')\n",
    "    print('#'*120)\n",
    "    print('Test Score')\n",
    "    test_score = model.evaluate(test_images_array, test_label_enc, batch_size=16, verbose = 1)\n",
    "    print('#'*120)\n",
    "    test_score_l.append(test_score)\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acc51479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fe21e9c5b0>,\n",
       " <matplotlib.lines.Line2D at 0x1fe21f34eb0>,\n",
       " <matplotlib.lines.Line2D at 0x1fe21f34fa0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAkHklEQVR4nO3deXxT15028OcnWd6wLQPejcEWELawmx0T0qGtzRJCkgbS7BshhQTebu+k7WTaTN82fdtJgYQmIQmhWQaaNJQSAqTtJFMMBILZ17DYJhgb2xjwgnf7zB+SZcmWbdlIulqe7+ejjyXdI+lncXk4nHPuvaKUAhER+T6d1gUQEZFrMNCJiPwEA52IyE8w0ImI/AQDnYjITwRp9cExMTEqNTVVq48nIvJJBw4cuKKUinW0TbNAT01NRU5OjlYfT0Tkk0TkQkfbOORCROQnGOhERH6iy0AXkRQR+VxETonICRFZ7qDNTBEpF5HDltvz7imXiIg64swYeiOAHyilDopIJIADIvJ3pdTJNu2ylVJzXV8iERE5o8seulKqSCl10HK/EsApAMnuLoyIiLqnW2PoIpIKYCyAfQ42TxGRIyKyXURGdPD6xSKSIyI5paWl3a+WiIg65HSgi0gEgI8ArFBKVbTZfBDAAKXUaAAvA9js6D2UUmuVUulKqfTYWIfLKImIqIecCnQRMcAc5u8rpTa13a6UqlBKVVnubwNgEJEYl1ZqcfVGPV74+CSq6hrd8fZERD7LmVUuAuAtAKeUUi910CbB0g4iMtHyvmWuLLTFrnNXsH5PHuaszsahr6+54yOIiHySMz30aQAeBPANm2WJs0VkiYgssbS5B8BxETkCYDWARcpNV864Y3QSNi6egsYmhXte+wKvfHYWTc28SAcRkWh1xaL09HR1M4f+l9c04N82H8eWI4WYmNoHLy0cjX69w11YIRGR9xGRA0qpdEfbfPZIUWOYAasWjcHvF47GyaIKZK3KxpYjhVqXRUSkGZ8NdAAQESwY2w/bl2dgcFwEnt1wCN//4DAqaxu0Lo2IyON8OtBbpPQJxwdPTcGKWYOx+dAlzF6djQMXOGFKRIHFLwIdAIL0OqyYdQs+XDIFSgH3vv4FVv3jLBqbmrUujYjII/wm0FuMH9AH25ZnYP7oJPz+H2ewcO1eXLxarXVZRERu53eBDgBRoQa8tHAMVi0agzOXKzF7VTY2H7qkdVlERG7ll4HeYv6YZGxbnoGhiZFY8afDWL7xECo4YUpEfsqvAx0wT5hueHIyfvDNW7D1aBGyVmZjf/5VrcsiInI5vw90wDxh+sy/DMafl0yBXidY+PoXeOlvX3HClIj8SkAEeoux/Xtj2/IMLBjbD6s/O4fvvP4FLpTd0LosIiKXCKhAB4CIkCD8572j8fJ9Y3G+pAqzV2XjowMF0OoUCERErhJwgd5i3ugkbF8xAyOSjfjBh0fwzIZDKK/mhCkR+a6ADXQASI4Ow4YnJ+NH3x6CHccvI2vVTuzLdctZf4mI3C6gAx0A9DrB0tsH4aOnpyI4SIdFb+zFbz89jQZOmBKRjwn4QG8xOiUanzybgXvHp2DN5+dxz6t7kHeFE6ZE5DsY6DZ6hQThN/eMwqv3j0N+WTXmrM7GB/svcsKUiHwCA92BrJGJ2LEiA6P7RePHHx3F0v86iOvV9VqXRUTUKQZ6BxKNYXjviUn416yh+PvJYmStysae81e0LouIqEMM9E7odYIltw3EpqenIcygx/1v7sOL20+jvpETpkTkfRjoThjZz4itz07Hogn98do/z+PuV/fgfGmV1mUREdlhoDspPDgIv75rJF5/cDwKrlVj7upd2PDl15wwJSKvwUDvpm+PSMCOFTMwfkBvPLfpGJa8dwDXbnDClIi0x0DvgfioULzz2ET8dPYwfHa6BJmrdmLXWU6YEpG2GOg9pNMJnpxhwual0xAZasADb+3Dr7adQl1jk9alEVGAYqDfpBFJRny8bDoemNwfa3fmYsGaPThXUql1WUQUgBjoLhAWrMcv7xyJNx9Kx+WKWsx9eRfe23uBE6ZE5FEMdBeaNTweO5ZnYEJqH/xs83E8+c4BlFXVaV0WEQUIBrqLxUWF4o+PTsTzc4dj55lSZK7Kxs4zpVqXRUQBgIHuBjqd4LHpafjrsmnoHW7AQ+u+xAsfn0RtAydMich9GOhuNCwxCluWTcfDUwZg3e483LlmN84Uc8KUiNyDge5moQY9fjH/Vrz9yARcqarDvJd34Z0v8jlhSkQux0D3kNuHxmH78hmYOrAvnv/rCTy2fj+ucMKUiFyIge5BsZEhWPfIBPzijhHYfb4MmSt34vOvSrQui4j8BAPdw0QED09NxcfLpiMmIgSPvr0fP99yghOmRHTTGOgaGZIQic1Lp+GxaWlYvycf81/ZjdOXK7Qui4h8GANdQ6EGPZ6fNxzrH52Ashv1uOOV3Vi3K48TpkTUIwx0LzBzSBw+XZGBjEExeGHrSTzy9n6UVNZqXRYR+RgGupfoGxGCNx9Ox3/ceSv25pYhc2U2/vtUsdZlEZEPYaB7ERHBg5MH4JNnpyM+KhSP/zEH/7b5OGrqOWFKRF3rMtBFJEVEPheRUyJyQkSWO2gjIrJaRM6JyFERGeeecgPDoLhIbF46FU9mpOHdvRcw75VdOFnICVMi6pwzPfRGAD9QSg0DMBnAUhEZ3qZNFoDBlttiAK+6tMoAFBKkx0/nDMe7j09ERU0D7lyzG29m56K5mROmRORYl4GulCpSSh203K8EcApAcptm8wG8o8z2AogWkUSXVxuAMgbHYseKGbhtSCx++ckpPPz2lyiu4IQpEbXXrTF0EUkFMBbAvjabkgFctHlcgPahTz3Up1cw1j44Hr9aMBL7868ic+VO/O3EZa3LIiIv43Sgi0gEgI8ArFBKtR3QFQcvaTc2ICKLRSRHRHJKS3mO8O4QEXx3Un9sfSYDyb3DsPjdA/jJX46hur5R69KIyEs4FegiYoA5zN9XSm1y0KQAQIrN434ACts2UkqtVUqlK6XSY2Nje1JvwBsUF4FNT0/DU7eZsOHLrzH35V04fqlc67KIyAs4s8pFALwF4JRS6qUOmm0B8JBltctkAOVKqSIX1kk2goN0eC5rGN5/fBKq65qw4A+78fo/z3PClCjAOdNDnwbgQQDfEJHDlttsEVkiIkssbbYByAVwDsAbAL7nnnLJ1tRBMdi+PAOzhsXj19tP44G39uFyOSdMiQKVaHXekPT0dJWTk6PJZ/sbpRQ+zCnAzz8+AYNeh9/cPRKZt3KREZE/EpEDSql0R9t4pKgfEBHcOyEFnzybgQF9w7HkvYP4v38+iht1nDAlCiQMdD+SFtMLHz09Fd+bORAfHLiIuS/vwpGL17Uui4g8hIHuZwx6HX6cORQbnpyMuoYm3P3qHqz5/ByaOGFK5PcY6H5qsqkvti+fgW/fmoDffvoVvvvGXhRer9G6LCJyIwa6HzOGG/DKfWPxu++MxvFL5chcuROfHOVqUiJ/xUD3cyKCe8b3wyfPZsAUG4Gl/3UQP/rwCKo4YUrkdxjoASI1phc+XDIFz35jED46WIA5q7Nx6OtrWpdFRC7EQA8gBr0O3//WEGxcPAWNTQr3vPYFXvnsLCdMifwEAz0ATUzrg23LMzBnZCJ+97czuG/tXhRcq9a6LCK6SQz0AGUMM2D1fWPx+4WjcbKoAlmrsrHlSLvzqRGRD2GgB7gFY/th+/IMDI6LwLMbDuH7fzqMytoGrcsioh5goBNS+oTjg6emYMWswdh8+BJmr87GgQucMCXyNQx0AgAE6XVYMesWfLhkCpQC7n39C6z8xxk0NjVrXRoROYmBTnbGD+iD7cszMH90Elb+4ywWrt2Li1c5YUrkCxjo1E5kqAEvLRyDVYvG4MzlSmStysZfDhVoXRYRdYGBTh2aPyYZ25ZnYFhiJP7Pn45g+cZDqOCEKZHXYqBTp1L6hGPj4in4wTdvwdajRchamY39+Ve1LouIHGCgU5f0OsEz/zIYf14yBXqdYOHrX+Clv32FBk6YEnkVBjo5bWz/3ti2PAN3jeuH1Z+dw3de+wIXym5oXRYRWTDQqVsiQoLwu++MxivfHYvc0irMXpWNPx8ogFbXpiWiVgx06pG5o5KwfcUMjEg24ocfHsEzGw6hvJoTpkRaYqBTjyVHh2HDk5Px48wh2HH8MrJW7cTe3DKtyyIKWAx0uil6neB7Mwfho6enIsSgx31v7MVvPz3NCVMiDTDQySVGp0Rj6zPTce/4FKz5/DzueXUP8q5wwpTIkxjo5DK9QoLwm3tG4dX7xyG/rBpzVmfjg/0XOWFK5CEMdHK5rJGJ2LEiA6P7RePHHx3F994/iOvV9VqXReT3GOjkFonGMLz/xCQ8lzUU/zhVjMyV2dhz/orWZRH5NQY6uY1OJ3jqtoHY9PQ0hAfrcf+b+/Di9tOob+SEKZE7MNDJ7Ub2M2Lrs9Nx38T+eO2f53HXq7txvrRK67KI/A4DnTwiPDgIv1owEq8/OB6XrtVg7upd2PDl15wwJXIhBjp51LdHJGDHihkYP6A3ntt0DE+9ewBXb3DClMgVGOjkcfFRoXjnsYn42Zxh+J+vSpG5cid2neWEKdHNYqCTJnQ6wRMZJvxl6VREhRnwwFv78P8+OYm6xiatSyPyWQx00tSIJCM+XjYdD0zujzey87BgzR6cK6nUuiwin8RAJ82FBevxyztH4s2H0nG5ohZzX96F9/Ze4IQpUTcx0MlrzBoejx0rMjAxrS9+tvk4nnwnB2VVdVqXReQzGOjkVeIiQ7H+kQl4fu5w7DxzBZmrsrHzTKnWZRH5BAY6eR2dTvDY9DT8ddk09A434KF1X+KFj0+itoETpkSdYaCT1xqWGIUty6bjkampWLc7D3eu2Y0zxZwwJepIl4EuIutEpEREjnewfaaIlIvIYcvtedeXSYEq1KDHz+8YgbcfmYArVXWY9/Iu/HFPPidMiRxwpoe+HkBmF22ylVJjLLcXbr4sInu3D43D9uUzMHVgX/z7lhN4bP1+lFZywpTIVpeBrpTaCeCqB2oh6lRsZAjWPTIBv7hjBHafL0PWqp34/HSJ1mUReQ1XjaFPEZEjIrJdREa46D2J2hERPDw1FVufmY6YiBA8un4/fr7lBCdMieCaQD8IYIBSajSAlwFs7qihiCwWkRwRySkt5VI06rlb4iOxeek0PDYtDev35GP+K7tx+nKF1mURaeqmA10pVaGUqrLc3wbAICIxHbRdq5RKV0qlx8bG3uxHU4ALNejx/LzhWP/oBJTdqMcdr+zGul15aG7mhCkFppsOdBFJEBGx3J9oec+ym31fImfNHBKHT1dkYMbgGLyw9SQeWb8fJZW1WpdF5HHOLFvcAOALAENEpEBEHheRJSKyxNLkHgDHReQIgNUAFimuKSMP6xsRgjceSsd/3Hkr9uWWIXNlNv77VLHWZRF5lGiVvenp6SonJ0eTzyb/dq6kEs9sOIxTRRV4cPIA/GT2MIQF67Uui8glROSAUird0TYeKUp+Z1BcJDYvnYonM9Lw7t4LmPfKLpwoLNe6LCK3Y6CTXwoJ0uOnc4bjvccnoaKmAQvW7MGb2bmcMCW/xkAnvzZ9cAx2rJiB24bE4pefnMLDb3+J4gpOmJJ/YqCT3+vTKxhrHxyPX981Ejn515C5cic+PXFZ67KIXI6BTgFBRHDfxP7Y+ux0JPcOw1PvHsBzm46hur5R69KIXIaBTgFlYGwENj09DU/dZsLG/V9j7su7cPwSJ0zJPzDQKeAEB+nwXNYwvP/4JFTXNWHBH3bjtX+e54Qp+TwGOgWsqYNisGNFBmYNi8eL20/jgbf2oai8RuuyiHqMgU4BLTo8GH+4fxz+/92jcPjidWSuzMb2Y0W8gAb5pCCtCyDSmojg3gkpmJDWB8s3HsLT7x9EZGgQBsdF4Jb4SAyy/LwlPhLxUSGwnLqIyOvw0H8iGw1NzfjLwUs4dqkcZ0sqcba4CmU36q3bGfSktc4O/WegE3WhrKoOZ0uqcLa4EmeKqxj0pKnOAp1DLkRd6BsRgr4RIZhs6mv3vKOg//vJYmzcf9HahkFPnsRAJ+ohBj15GwY6kYsx6EkrDHQiD2HQk7sx0Ik0xqAnV2GgE3kpBj11FwOdyMcw6KkjDHQiP8GgJwY6kZ9j0AcOBjpRgGLQ+x8GOhHZcVXQD46PtN5n0HsGA52InMKg934MdCK6KZ0F/ZniKpwrMQf9mWIGvbsx0InILfpGhGBKRAimDGTQewoDnYg8ikHvPgx0IvIKDPqbx0AnIq/GoHceA52IfBKDvj0GOhH5lUAOegY6EQWEQAh6BjoRBTR/CnoGOhGRA74Y9Ax0IqJucEXQL5rQH/dOSHF5bQx0IiIX6E7Q1zU2uaUGBjoRkRt1FPTuoHP7JxARkUcw0ImI/AQDnYjIT3QZ6CKyTkRKROR4B9tFRFaLyDkROSoi41xfJhERdcWZHvp6AJmdbM8CMNhyWwzg1Zsvi4iIuqvLQFdK7QRwtZMm8wG8o8z2AogWkURXFUhERM5xxRh6MoCLNo8LLM+1IyKLRSRHRHJKS0td8NFERNTCFYHu6DhW5aihUmqtUipdKZUeGxvrgo8mIqIWrgj0AgC2x7D2A1DogvclIqJucMWRolsALBORjQAmAShXShW54H2JiLyeUgp1TXWobqxGdUM1ahprrPdtn6tprLE+lx6fjttSbnN5LV0GuohsADATQIyIFAD4dwAGyy/yGoBtAGYDOAegGsCjLq+SiMgFGpobWkO3k/Dt7DlH95tVs9M1hOpDEaIP0SbQlVL3dbFdAVjqsoqIKOA1q2a7Hq3TQeuonc32huYGp2sIkiCEG8LNt6BwhAWFIdwQjrjwOIQHmZ8PCwqzPm/7XMv9ts+FBYVBr9O77XvjybmIqMfaDjd0K2htn2vTtrap1ukaBGIXni3BaQw1IjEo0amgdfScQW9w4zfnHgx0ogDR0NSA6sbqbvd8WwK3pqGm3fM1jTXdHm6whqZN+PYN7eswlB21bdsuRO9dl4HTEgOdyMs0NTe1TqI5Ob5rF9KW8LXt+VY3VqOxudHpGgw6g30P1hKg8eHxTgdt2+dC9aFuHW4gBjpRjymlUNtU6zBoWwK1y96wg1DuznCDTnTWwA0ztI7TRodGIykoyamgbdcb9tHhBmKgEwEAbjTcQF55HnLLc5Ffno/K+kq78d2OAlo5PobOIesEmk349grqhdiwWIdjum1D1tE4L4cbyBYDnQKGUgpXa68itzzXGt6513ORW56L4upiazu96BERHGE31BAWFIaE8ARrEHd3ko3DDeQJDHTyO82qGUU3iqxhbQ3v8lyU15Vb24UFhcFkNGFiwkSYok1IM6bBZDShX2Q/GHQcciDfw0Ann9XQ3ICLFRetYd3S486vyEdNY421XZ/QPkgzpuFbA74Fk9FkvkWbEB8ez+EK8isMdPJ61Q3VyKvIQ+51+972xYqLaFStKzcSeyXCZDQhPSG9NbiNJkSHRmtXPPm/pgagttxyuw7UXLd5bHnO9nHNdWDUQmDSYpeXwkAnr3Gt9ppdT7slvItutJ4aKEiCkBKVApPRhFn9Z5mHSaJNSItKQ7ghXMPqyWc1NwF1FfaB21Ug2z5uuNH5++uCgNBoINQIhFl+BrtnX2Wgk0cppXD5xuV2wyR55Xm4VnfN2i4sKAypUakYFz/OrredEpXC8W2ypxRQX9XzQLaZV3FMzCFsG8gxgyzPRbeGte126y0aMIQBHhraY6CTWzQ0N+Bi5UXkXc+zC++88jy78e3okGiYjCZ8o/83rGPbJqMJCb0SoBNewzwgKAU01nYQuNe7DuTackA1df4ZwZH2gRudAoTeah/GHQVycASg8419kYFON6WmscZuCWDL/a8rv7Y7MjGhVwJMRhPuHny3dTWJKdqEPqF9NKyeXKax3jxsYQ3btj+76D031Xf+/kFh9oEbEQfEDHYukEOiAH1gRF1g/JZ0067XXrcfJinPRd71PBTeaL2WiV70SIlMse9xG83LATm+7eVaxpG7O1zRsr2huvP31xnah210/zbhaxvItj+jgKAQ9/7+foKBTlZKKRRXF1vXb9sOk1ytbb1OeKg+FGnGNIyJG4O7jHdZh0n6R/bnIeNaaRlH7mkg11V0/v6iax++MYPte8KOesct9z04jhzIGOgBqLG5EQWVBXaB3RLi1Y2tPa2o4CiYjCbMTJlp7WmbjCYkRSRxfNvVWsaROwzk611P+HV11sOQKAc95JGdD1e03PehceRAxkD3Y7WNtcivyLfrceeV5+FCxQW7E/3HhcfBZDThzkF3Wse204xp6BvalwfedEdjvYOwve5cD9mZcWRDuH3gRsQDMbc4F8gBNI4cyPgn7AfK68rbnZsktzwXhVWF1pNH6USHlMgUpBnTMKPfDLvx7YjgCI1/Ay+iFFBeAFSXdW/IokfjyNH248jtArm3zX2OI1PXGOg+QimFkuoS+2ESS4CX1ZZZ24XoQ5AalYpRMaMwf9B8a3APiBqAYH2whr+BF1IKqCwCLh0ECg8ChYfMt5prjtu3G0eOdtBDjnbcQ+Y4MnkAA93LNDU3oaCqoN0wSV55HqoaqqztIoMjYTKaWnvblmGSpF5JPKtfR26UtQZ3S4hXWc6yKHogbjgwbB6QOAaITGgfyCGRDGTyagx0jdQ11SG/PN/uaMnc8tx249uxYbEwRZswb+A8uxNLcXy7C7XlQNER+9739a8tG8W8QsN0O5A0FkgeBySMNPegiXwYA93NKuor2p1UKvd6Li5VXbIb306OSIbJaEJGcob1/CQmowmRwZEa/wY+oL4auHzMvvdddrZ1e/QAIHk8MOEJIGkckDjaPCZN5GcY6C6glEJpTaldT7slwK/UXLG2C9YFY4BxAEbEjLD2uNOMaUg1piJEzwkvpzTWAyUnLD1vy5h3yanWQ78jE8297lELgeSxQOJYoFdfbWsm8hAGejc0NTehsKrQ4RGTlQ2V1nYRhgiYjCZMS5pm7WmbjCYkRyRzfLs7mpuA0q8swX3QHOLFx1uX94X1Nve4h2SZQzxpHBCVqG3NRBpioDtQ31RvXr9tCeuW4M4vz0d9c+ta4ZiwGJiMJsw2zbY7sVRsWCzHt7tLKeBqrs2E5SHzGHjLqUmDI4GkMcCkp8zBnTzOPJTC75nIKqADvaq+ql1PO7c8FwVVBWi2HHUnEPP4drQJU5OmWodJ0oxpMIYYNf4NfJRSQMUlm2ETy89ay2lMg0KBhFHA2AfMwZ00Fug7mEcqEnXB7wNdKYWy2rL25ye5noeSmhJrO4POgAFRAzC0z9DWHrdl/XZoUKiGv4EfqCq1HzYpPATcsHz3uiDzcsERC8w976SxQNwwgOeEIeo2vwn0ZtWMS1WX7M5L0nKrrG8d3+5l6IW0qDRMTppsPTfJwOiBSI5IRpDOb74O7dRcB4oO209all+0bBQgdggwaFbrcsH4WwED/8EkcgWfS7CGpgbr+LbtMEl+RT7qmuqs7fqE9oHJaEJWapbdFd15YWAXqr8BFB21731fPd+6vXca0G9C67h34ijzwTlE5BY+F+g78nfgJ7t+Yn2cHJGMNGMaJiVOspuY5Pi2izXWmVeYFB4CLlkCvPR06xn+IpPMPe4x37WsOBkLhPPiFUSe5HOBPiFhAl7MeBEmowmpxlSEBfHoPpdragSufGV/lOXl40DLEazhfc097qFzWyctIxO0rZmIfC/QE3olYI5pjtZl+I/mZstyQZujLC8fbT1zYEiU+cjKKd9rnbSM7s/lgkReyOcCnW6CUuYJStuTUxUeab3qeVCYeZx73MOtk5Z9BnK5IJGPYKD7s6oS+2GTSweBasupCHQGIH4EMPLu1qMsY4fyIghEPox/e/1FzTX7oywLD5kP3gHM5/GOHQrc8m375YK8YAKRX2Gg+6K6KvNh8bbLBa/ltW7vYwL6T2mdsEwYBYTwqkRE/o6B7u0aaoHiE/ZHWV75qnW5YFQ/81kFxz1ombQcYz5pFREFHAa6N2lqBEpP2Z/jpPikzXLBGHOve/j81t53RJy2NROR12Cga6W5GSg7Zz9scvkY0Fhj3h5iNPe2py5rXS5o7MflgkTUIacCXUQyAawCoAfwplLqxTbbZwL4K4CWgdxNSqkXXFemj1PKfPkz22GToiNAXYV5uyHcvNY7/bHWScveaVwuSETd0mWgi4gewBoA3wRQAGC/iGxRSp1s0zRbKTXXDTX6nsrL7U8NW11m3qYPNq8wGfkdy7DJOPOV47lckIhukjMpMhHAOaVULgCIyEYA8wG0DfTAVH3VZp23ZblgZaF5m+iA2GGWK+pYhk3iR3C5IBG5hTOBngzgos3jAgCTHLSbIiJHABQC+KFS6kTbBiKyGMBiAOjfv3/3q9VaXWX7K8lfy2/d3ncQkDrd5kryo4DgcM3KJaLA4kygO5qFU20eHwQwQClVJSKzAWwGMLjdi5RaC2AtAKSnp7d9D+/SUNv+SvJXzsD6qxv7myctxz/SeiX5sGjt6iWigOdMoBcASLF53A/mXriVUqrC5v42EfmDiMQopa7AFzQ1ACUn7c9xUnIKaG40b+8VZ+5x33q3+WfiGCAiVtOSiYjacibQ9wMYLCJpAC4BWATgu7YNRCQBQLFSSonIRAA6AGWuLtYlmpvMywXtTg17DGisNW8PjTYPmUxbbnMl+SQuFyQir9dloCulGkVkGYBPYV62uE4pdUJElli2vwbgHgBPi0gjgBoAi5RS2g+pKGUe47adtCw6DNRXmbcbepmHTSY8Yb9ckOFNRD5ItMrd9PR0lZOT49o3rShsv1yw5pp5mz4YSBhp7nG3HGUZcwug07u2BiIiNxKRA0qpdEfbfHfx842y9leSr7ps3iZ685Xkh81rHTaJGw4EBWtbMxGRG/leoJ/5FNj2Q/ORlwAAAWIGA6aZ9qeG5XJBIgowvhfoEXHmHveEJ1qXC4ZGaV0VEZHmfC/Qk8YC9/5R6yqIiLwOz/5EROQnGOhERH6CgU5E5CcY6EREfoKBTkTkJxjoRER+goFOROQnGOhERH5Cs5NziUgpgAs9fHkMAG8817q31gV4b22sq3tYV/f4Y10DlFIOL8igWaDfDBHJ6ehsY1ry1roA762NdXUP6+qeQKuLQy5ERH6CgU5E5Cd8NdDXal1AB7y1LsB7a2Nd3cO6uieg6vLJMXQiImrPV3voRETUBgOdiMhPeF2gi0imiHwlIudE5F8dbBcRWW3ZflRExjn7WjfXdb+lnqMiskdERttsyxeRYyJyWERcemVsJ+qaKSLlls8+LCLPO/taN9f1I5uajotIk4j0sWxz5/e1TkRKROR4B9u12r+6qkur/aururTav7qqy+P7l4ikiMjnInJKRE6IyHIHbdy7fymlvOYGQA/gPAATgGAARwAMb9NmNoDtAATAZAD7nH2tm+uaCqC35X5WS12Wx/kAYjT6vmYC2NqT17qzrjbt5wH4zN3fl+W9ZwAYB+B4B9s9vn85WZfH9y8n6/L4/uVMXVrsXwASAYyz3I8EcMbT+eVtPfSJAM4ppXKVUvUANgKY36bNfADvKLO9AKJFJNHJ17qtLqXUHqXUNcvDvQD6ueizb6ouN73W1e99H4ANLvrsTimldgK42kkTLfavLuvSaP9y5vvqiKbfVxse2b+UUkVKqYOW+5UATgFIbtPMrfuXtwV6MoCLNo8L0P4L6aiNM691Z122Hof5X+EWCsDfROSAiCx2UU3dqWuKiBwRke0iMqKbr3VnXRCRcACZAD6yedpd35cztNi/ustT+5ezPL1/OU2r/UtEUgGMBbCvzSa37l/edpFocfBc23WVHbVx5rU95fR7i8jtMP+Fm27z9DSlVKGIxAH4u4ictvQwPFHXQZjP/VAlIrMBbAYw2MnXurOuFvMA7FZK2fa23PV9OUOL/ctpHt6/nKHF/tUdHt+/RCQC5n9AViilKtpudvASl+1f3tZDLwCQYvO4H4BCJ9s481p31gURGQXgTQDzlVJlLc8rpQotP0sA/AXm/155pC6lVIVSqspyfxsAg4jEOPNad9ZlYxHa/HfYjd+XM7TYv5yiwf7VJY32r+7w6P4lIgaYw/x9pdQmB03cu3+5emLgZm4w/48hF0AaWicGRrRpMwf2kwpfOvtaN9fVH8A5AFPbPN8LQKTN/T0AMj1YVwJaDyCbCOBry3en6fdlaWeEeRy0lye+L5vPSEXHk3we37+crMvj+5eTdXl8/3KmLi32L8vv/Q6AlZ20cev+5bIv14V/SLNhnh0+D+CnlueWAFhi86WtsWw/BiC9s9d6sK43AVwDcNhyy7E8b7L84RwBcEKDupZZPvcIzJNpUzt7rafqsjx+BMDGNq9z9/e1AUARgAaYe0WPe8n+1VVdWu1fXdWl1f7VaV1a7F8wD4MpAEdt/pxme3L/4qH/RER+wtvG0ImIqIcY6EREfoKBTkTkJxjoRER+goFOROQnGOhERH6CgU5E5Cf+F4dZvReWgP86AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_score_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd6d7eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2.470787286758423, 0.41555556654930115, 0.6267337203025818],\n",
       " [1.3434394598007202, 0.7410112619400024, 0.8802769780158997],\n",
       " [1.0797243118286133, 0.8414364457130432, 0.9532791972160339]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48253615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(74+84)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f918961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0571595430374146,\n",
       " 1.0335742235183716,\n",
       " 1.0118024349212646,\n",
       " 0.985831081867218,\n",
       " 0.9746829867362976,\n",
       " 0.951061487197876,\n",
       " 0.9395124316215515,\n",
       " 0.9262233972549438,\n",
       " 0.9091644883155823,\n",
       " 0.8996343612670898,\n",
       " 0.888978123664856,\n",
       " 0.8783875107765198,\n",
       " 0.8640640377998352,\n",
       " 0.8602203726768494,\n",
       " 0.8475135564804077]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf2d80e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.311429500579834,\n",
       " 0.3648974299430847,\n",
       " 0.40781503915786743,\n",
       " 0.45418429374694824,\n",
       " 0.4846629798412323,\n",
       " 0.5254966020584106,\n",
       " 0.5510257482528687,\n",
       " 0.5831976532936096,\n",
       " 0.6091175675392151,\n",
       " 0.6381634473800659,\n",
       " 0.6553565859794617,\n",
       " 0.6819928288459778,\n",
       " 0.6965157985687256,\n",
       " 0.7234125733375549,\n",
       " 0.7394334077835083]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist[0].history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b60407fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7089430689811707,\n",
       " 0.7310569286346436,\n",
       " 0.7500487565994263,\n",
       " 0.7627967596054077,\n",
       " 0.7754146456718445,\n",
       " 0.7871219515800476,\n",
       " 0.8072845339775085,\n",
       " 0.8243902325630188,\n",
       " 0.8330406546592712,\n",
       " 0.8338862061500549,\n",
       " 0.8477398157119751,\n",
       " 0.8606829047203064,\n",
       " 0.865821123123169,\n",
       " 0.875382125377655,\n",
       " 0.8813658356666565]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist[1].history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab3476f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(test_images_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e35adec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_bool = np.argmax(y_predict,axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5947858d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[435,   0,   0,   0],\n",
       "       [  0, 150,   0,   0],\n",
       "       [  0,   0, 600,   0],\n",
       "       [  0,   0,   2, 598]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_image_label,y_predict_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7ada193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9988832742399285"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(test_image_label,y_predict_bool,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38c51f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99915611814346\n"
     ]
    }
   ],
   "source": [
    "fpr,tpr,thresholds = roc_curve(test_image_label,y_predict_bool,pos_label=3)\n",
    "print(auc(fpr,tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad042b07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

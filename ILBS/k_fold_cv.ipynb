{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b27676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization, MaxPool2D\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d597772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abdfa525",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image = []\n",
    "all_label = []\n",
    "def vol_(path):\n",
    "    for index,folder in enumerate(os.listdir(path)):\n",
    "        print(index,folder)\n",
    "        image = []\n",
    "        for row in os.listdir(path+'\\\\'+folder):\n",
    "            pat_path = path+'\\\\'+folder+'\\\\'+row\n",
    "            all_image.append(pat_path)\n",
    "            all_label.append(index)\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97b6e69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 00_5\n",
      "1 06_10\n",
      "2 11_16\n",
      "3 17_28\n"
     ]
    }
   ],
   "source": [
    "image_train_set = vol_(r'C:\\Users\\AIIMS-IITD\\Desktop\\Sanjeev\\4_class_cluster_volume\\Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a0bb032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_maps(path,label):\n",
    "    #print(path[0])\n",
    "    images = []\n",
    "    for index,item in enumerate(path):\n",
    "        for name in enumerate(os.listdir(item)):\n",
    "            #print(name[1])\n",
    "            folder = os.path.join(item, name[1])\n",
    "            #print('folder: ',folder)\n",
    "            img = cv2.imread(folder)\n",
    "            #print(img.shape)\n",
    "            if img is not None:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "                img1 = cv2.resize(img, (196, 196))\n",
    "                img = np.dstack((img1,img1,img1))\n",
    "            if img is not None:\n",
    "           #     img = (img-np.mean(img))/np.std(img)\n",
    "                images.append((np.array(img),label[index]))\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1959756",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(input_shape=(196,196,3),filters=64,kernel_size=(5,5),padding='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(2,2))\n",
    "\n",
    "model.add(Conv2D(filters=32,kernel_size=(3,3),padding='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters=16,kernel_size=(3,3),padding='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(MaxPooling2D(2,2))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten(name='flatten'))\n",
    "#model.add(Dense(16, activation='relu', name='fc1'))\n",
    "model.add(Dense(4, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "232094c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "480/480 [==============================] - 21s 38ms/step - loss: 2.8021 - accuracy: 0.3140 - auc: 0.5657: 17s - loss: 3.1192 - accuracy: 0.2461 - auc: - ETA: 4s - loss: 2 - ETA: 3s - loss: 2.8364 - accuracy: 0.3066 - ETA: 3s - loss: 2.8302 - accuracy: 0.3082 - auc: 0.55 - ETA: 3s - loss: 2.8284 - ac - ETA: 2s - loss: 2.8169 - accuracy: 0.3099 - auc: 0.56 - ETA: 2s - loss: 2.8161 - accura - ETA: 1s - loss: 2.8097 - accu - ETA: 0s - loss: 2.8046 - accuracy: 0.3132 -\n",
      "Epoch 2/15\n",
      "480/480 [==============================] - 18s 38ms/step - loss: 2.5827 - accuracy: 0.3686 - auc: 0.6190: 16s - loss: 2.5903 - accuracy: 0.3575 - au - ETA: 15s - loss: 2.6115 - accuracy: 0.3519 - auc: 0.6 - ETA: 15s - loss: 2.6107 - accuracy: 0.3571 - auc - ETA: 14s - loss: 2.6136 - accuracy: 0.3566 - auc - ETA: 14s - loss: 2.6078 - accuracy: 0.3566 - auc: 0.613 - ETA: 14s - loss: 2.6064 - accuracy: 0.35 - - ETA: 7s - loss: 2.6071 - accuracy: 0.3599 - a - ETA: 6s - loss: 2.6072 - accuracy: 0.3617 - auc: 0. - ETA: 6s - loss: 2.6053 - accura - ETA: 3s - loss: 2.6015 - accuracy: 0.3637 - auc - ETA: 3s - -\n",
      "Epoch 3/15\n",
      "480/480 [==============================] - 18s 38ms/step - loss: 2.4051 - accuracy: 0.4112 - auc: 0.6552\n",
      "Epoch 4/15\n",
      "480/480 [==============================] - 18s 38ms/step - loss: 2.2791 - accuracy: 0.4403 - auc: 0.6815\n",
      "Epoch 5/15\n",
      "480/480 [==============================] - 18s 38ms/step - loss: 2.2201 - accuracy: 0.4615 - auc: 0.69830s - loss: 2.2231 - accura\n",
      "Epoch 6/15\n",
      "480/480 [==============================] - 18s 38ms/step - loss: 2.1069 - accuracy: 0.4929 - auc: 0.7218\n",
      "Epoch 7/15\n",
      "480/480 [==============================] - 18s 38ms/step - loss: 2.0209 - accuracy: 0.5169 - auc: 0.73941s - loss: 2.0194 - accuracy:  - ETA: 0s - loss: 2.0188 - accuracy: 0.\n",
      "Epoch 8/15\n",
      "480/480 [==============================] - 18s 38ms/step - loss: 1.9504 - accuracy: 0.5405 - auc: 0.75560s - loss: 1.9525 - accuracy: 0.5399 - a\n",
      "Epoch 9/15\n",
      "480/480 [==============================] - 18s 38ms/step - loss: 1.8864 - accuracy: 0.5591 - auc: 0.7714\n",
      "Epoch 10/15\n",
      "480/480 [==============================] - 18s 38ms/step - loss: 1.8053 - accuracy: 0.5875 - auc: 0.78763s - l\n",
      "Epoch 11/15\n",
      "480/480 [==============================] - 18s 38ms/step - loss: 1.7672 - accuracy: 0.5986 - auc: 0.79601s - l\n",
      "Epoch 12/15\n",
      "480/480 [==============================] - 18s 38ms/step - loss: 1.7208 - accuracy: 0.6167 - auc: 0.80610s - loss: 1.7246 - accuracy: 0.6154 - auc\n",
      "Epoch 13/15\n",
      "480/480 [==============================] - 18s 38ms/step - loss: 1.6700 - accuracy: 0.6311 - auc: 0.81703s - loss: 1.6717 - accuracy: 0.6311 - auc:  - ETA: 3s - l\n",
      "Epoch 14/15\n",
      "480/480 [==============================] - 18s 38ms/step - loss: 1.5958 - accuracy: 0.6567 - auc: 0.83341s -\n",
      "Epoch 15/15\n",
      "480/480 [==============================] - 18s 38ms/step - loss: 1.5527 - accuracy: 0.6739 - auc: 0.8403\n",
      "57/57 [==============================] - 1s 13ms/step - loss: 2.6567 - accuracy: 0.2511 - auc: 0.5936\n",
      "Epoch 1/15\n",
      "481/481 [==============================] - 19s 39ms/step - loss: 1.6956 - accuracy: 0.6346 - auc: 0.81780s - loss: 1.6961 - accuracy: 0.6345 -\n",
      "Epoch 2/15\n",
      "481/481 [==============================] - 18s 38ms/step - loss: 1.6059 - accuracy: 0.6619 - auc: 0.83320s - loss: 1.6081 - accuracy: \n",
      "Epoch 3/15\n",
      "481/481 [==============================] - 18s 38ms/step - loss: 1.5393 - accuracy: 0.6842 - auc: 0.8472\n",
      "Epoch 4/15\n",
      "481/481 [==============================] - 18s 37ms/step - loss: 1.5121 - accuracy: 0.6940 - auc: 0.8530: 13s - loss: 1.4 - ETA: 11s - loss: 1.5011 - accuracy: 0. - ETA: 9s - loss: 1.5034  - E - - ETA: 2s - los - ETA: 1s - loss: 1.5\n",
      "Epoch 5/15\n",
      "481/481 [==============================] - 18s 37ms/step - loss: 1.4504 - accuracy: 0.7112 - auc: 0.8655\n",
      "Epoch 6/15\n",
      "481/481 [==============================] - 18s 38ms/step - loss: 1.4218 - accuracy: 0.7200 - auc: 0.8699\n",
      "Epoch 7/15\n",
      "481/481 [==============================] - 18s 38ms/step - loss: 1.3951 - accuracy: 0.7302 - auc: 0.8762\n",
      "Epoch 8/15\n",
      "481/481 [==============================] - 18s 38ms/step - loss: 1.3496 - accuracy: 0.7461 - auc: 0.8852\n",
      "Epoch 9/15\n",
      "481/481 [==============================] - 18s 37ms/step - loss: 1.3368 - accuracy: 0.7520 - auc: 0.88701s - loss: 1.3386 - \n",
      "Epoch 10/15\n",
      "481/481 [==============================] - 18s 37ms/step - loss: 1.2883 - accuracy: 0.7696 - auc: 0.8973\n",
      "Epoch 11/15\n",
      "481/481 [==============================] - 18s 38ms/step - loss: 1.2652 - accuracy: 0.7774 - auc: 0.9009\n",
      "Epoch 12/15\n",
      "481/481 [==============================] - 18s 37ms/step - loss: 1.2394 - accuracy: 0.7908 - auc: 0.90727s - loss: 1.2435 - accuracy: 0.7947 - auc: 0. - ETA: 6s - loss: 1.2450 - accura\n",
      "Epoch 13/15\n",
      "481/481 [==============================] - 18s 38ms/step - loss: 1.1997 - accuracy: 0.8040 - auc: 0.91371s - los\n",
      "Epoch 14/15\n",
      "481/481 [==============================] - 18s 37ms/step - loss: 1.1838 - accuracy: 0.8071 - auc: 0.91681s -\n",
      "Epoch 15/15\n",
      "481/481 [==============================] - 18s 37ms/step - loss: 1.1688 - accuracy: 0.8160 - auc: 0.91941s - loss: 1.1716 \n",
      "56/56 [==============================] - 1s 15ms/step - loss: 1.7525 - accuracy: 0.5708 - auc: 0.7833\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[32,64,196,196] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node sequential/batch_normalization/FusedBatchNormV3 (defined at <ipython-input-7-450618c6cc0c>:34) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[assert_less_equal/Assert/AssertGuard/pivot_f/_13/_41]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[32,64,196,196] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node sequential/batch_normalization/FusedBatchNormV3 (defined at <ipython-input-7-450618c6cc0c>:34) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_48803]\n\nFunction call stack:\ntrain_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-450618c6cc0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdadelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m               metrics=['accuracy','AUC'])\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label_enc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mtest_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_images_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_label_enc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[32,64,196,196] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node sequential/batch_normalization/FusedBatchNormV3 (defined at <ipython-input-7-450618c6cc0c>:34) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[assert_less_equal/Assert/AssertGuard/pivot_f/_13/_41]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[32,64,196,196] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node sequential/batch_normalization/FusedBatchNormV3 (defined at <ipython-input-7-450618c6cc0c>:34) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_48803]\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "images_ = all_image\n",
    "labels_ = all_label\n",
    "hist = []\n",
    "test_score_l = []\n",
    "for j, (train_idx, test_idx) in enumerate(folds.split(images_, labels_)):\n",
    "    X_train = np.array(images_)[train_idx]\n",
    "    y_train = np.array(labels_)[train_idx]\n",
    "    X_test = np.array(images_)[test_idx]\n",
    "    y_test = np.array(labels_)[test_idx]\n",
    "    \n",
    "    train_image = import_maps(X_train,y_train)\n",
    "    test_image = import_maps(X_test,y_test)\n",
    "    \n",
    "    train_images_all = [i[0] for i in train_image]\n",
    "    train_images_array = np.array(train_images_all)\n",
    "    #train_images_array=np.expand_dims(train_images_array,axis=3)\n",
    "    train_image_label = [i[1] for i in train_image]\n",
    "    train_image_label = np.array(train_image_label)\n",
    "    \n",
    "    test_images_all = [i[0] for i in test_image]\n",
    "    test_images_array = np.array(test_images_all)\n",
    "    #test_images_array=np.expand_dims(test_images_array,axis=3)\n",
    "    test_image_label = [i[1] for i in test_image]\n",
    "    test_image_label = np.array(test_image_label)\n",
    "    \n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    train_label_enc = enc.fit_transform(train_image_label.reshape(-1, 1)).toarray()\n",
    "    #val_label_enc = enc.fit_transform(val_y.reshape(-1, 1)).toarray()\n",
    "    test_label_enc = enc.fit_transform(test_image_label.reshape(-1, 1)).toarray()\n",
    "    \n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy','AUC'])\n",
    "    history = model.fit(train_images_array, train_label_enc, batch_size=32, epochs=15, verbose=1,shuffle=True)\n",
    "    hist.append(history)\n",
    "    test_score = model.evaluate(test_images_array, test_label_enc, batch_size=32, verbose = 1)\n",
    "    test_score_l.append(test_score)\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acc51479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2a7f472f0d0>,\n",
       " <matplotlib.lines.Line2D at 0x2a7f472f190>,\n",
       " <matplotlib.lines.Line2D at 0x2a7f472f250>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_score_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd6d7eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2.6566829681396484, 0.25111111998558044, 0.5936233401298523],\n",
       " [1.7525477409362793, 0.5707865357398987, 0.7832502126693726]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cc7f00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
